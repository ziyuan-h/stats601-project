{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pandas_datareader as pdr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import bs4 as bs\n",
    "import requests\n",
    "from IPython.display import clear_output\n",
    "from scipy.stats import mstats\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import RandomizedSearchCV, validation_curve, TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "sns.set()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## prepare for the data\n",
    "all_data['Close_Shifted'] = all_data.groupby('symbol')['Close'].transform(lambda x: x.shift(-6))\n",
    "all_data['Target'] = (all_data['Close_Shifted'] - all_data['Open'])#((all_data['Close_Shifted'] - all_data['Open'])/(all_data['Open']) * 100).shift(-1)\n",
    "all_data['Target_Direction'] = np.where(all_data['Target']>0,1,0)\n",
    "all_data = all_data.dropna().copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "## creating clusters\n",
    "# features 10 * feature numpy array\n",
    "#features = np.ones((10,20))\n",
    "# limit the extreme\n",
    "Target_variables = ['SMA_ratio','ATR_5','ATR_15','ATR_Ratio',\n",
    "                       'ADX_5','ADX_15','SMA_Volume_Ratio','Stochastic_5','Stochastic_15','Stochastic_Ratio',\n",
    "                      'RSI_5','RSI_15','RSI_ratio','MACD']\n",
    "for variable in Target_variables:\n",
    "    all_data.loc[:,variable] = mstats.winsorize(all_data.loc[:,variable], limits = [0.1,0.1])\n",
    "# for i in range(features.shape[1]): # each column of features\n",
    "#     features[:,i] =mstats.winsorize((features[:,i]), limits=[0.1,0.1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1.]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##clustering companies\n",
    "#Extract the returns\n",
    "returns = all_data[['symbol','return']].copy()\n",
    "returns['Date'] = returns.index.copy()\n",
    "\n",
    "#Pivot the returns to create series of returns for each stock\n",
    "transposed = returns.pivot(index = 'Date', columns = 'symbol', values = 'return')\n",
    "\n",
    "#Transpose the data to get companies on the index level and dates on the column level since clusters takes place on index level\n",
    "X = transposed.dropna().transpose()\n",
    "\n",
    "#Extract sum of squares for K-means clusters from 1 to 50 clusters\n",
    "sum_of_sq = np.zeros([10, 1])\n",
    "for k in range(1, 10):\n",
    "    sum_of_sq[k-1] = KMeans(n_clusters=k).fit(X).inertia_\n",
    "\n",
    "plt.plot(range(1, 10), sum_of_sq[1:10])\n",
    "plt.title(\"Elbow Method\")\n",
    "plt.xlabel(\"Number of Cluster\")\n",
    "plt.ylabel(\"Within-cluster Sum of Squares\")\n",
    "\n",
    "pd.DataFrame(sum_of_sq, columns = ['Difference in SS'], index = range(1,10)).diff()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Get 3 clusters #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "gmm = GaussianMixture(n_components = 3)\n",
    "gmm.fit(transposed.dropna().transpose())\n",
    "\n",
    "#Predict for each company\n",
    "clusters = gmm.predict(transposed.dropna().transpose())\n",
    "clusters_df = pd.DataFrame({'Cluster':clusters,\n",
    "                           'Companies':transposed.columns})\n",
    "\n",
    "#Sort by Clusters\n",
    "clusters_df = clusters_df.sort_values(['Cluster']).reset_index(drop = True)\n",
    "\n",
    "#Save as csv\n",
    "clusters_df.to_csv(\"clusters.csv\")\n",
    "clusters_df = pd.read_csv(\"clusters.csv\", index_col = 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_data.index = pd.to_datetime(all_data.index)\n",
    "\n",
    "train_data = all_data.loc[:'2018-12-31',]#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "test_data = all_data.loc['2019-01-01':] #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# select n_estimators\n",
    "#Separate between X and Y\n",
    "X_train = train_data.loc[:,Target_variables]\n",
    "\n",
    "Y_train = train_data.loc[:,['Target_Direction']]\n",
    "\n",
    "#Create validation curve for the Random Forest Classifier\n",
    "rf = RandomForestRegressor()#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "train_scoreNum, test_scoreNum = validation_curve(rf,\n",
    "                                X = X_train['2010-01-01':], y = Y_train.loc['2010-01-01':,'Target_Direction'],\n",
    "                                param_name = 'n_estimators',\n",
    "                                param_range = [3,4,7,10,12,15,20,25,30], cv = TimeSeriesSplit(n_splits = 3))\n",
    "\n",
    "train_scores_mean = np.mean(train_scoreNum, axis=1)\n",
    "train_scores_std = np.std(train_scoreNum, axis=1)\n",
    "test_scores_mean = np.mean(test_scoreNum, axis=1)\n",
    "test_scores_std = np.std(test_scoreNum, axis=1)\n",
    "\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.plot([3,4,7,10,12,15,20,25,30],train_scores_mean)\n",
    "plt.plot([3,4,7,10,12,15,20,25,30],test_scores_mean)\n",
    "plt.legend(['Train Score','Test Score'], fontsize = 'large')\n",
    "plt.title('Validation Curve Score for n_estimators', fontsize = 'large')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# build Random Forest Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Run the loop for every unique cluster - 17 loops\n",
    "for cluster_selected in clusters_df.Cluster.unique():\n",
    "\n",
    "    print(f'The current cluster running is : {cluster_selected}')\n",
    "\n",
    "    #Get data for that cluster\n",
    "    co_data = all_data[all_data.symbol.isin(clusters_df.loc[clusters_df.Cluster==cluster_selected,'Companies'].tolist())].copy()\n",
    "    co_train = co_data[:'2018-12-31']\n",
    "    co_train = co_train.dropna().copy()\n",
    "\n",
    "    X_train = co_train.loc[:,Target_variables]\n",
    "\n",
    "    Y_train = co_train.loc[:,['Target_Direction']]\n",
    "\n",
    "    #Define paramters from Validation Curve\n",
    "    params = {'max_depth': [5, 7],\n",
    "          'max_features': ['sqrt'],\n",
    "          'min_samples_leaf': [10, 15, 20],\n",
    "          'n_estimators': [5, 7, 9],\n",
    "         'min_samples_split':[20, 25, 30]} #Using Validation Curves\n",
    "\n",
    "    rf = RandomForestClassifier()\n",
    "\n",
    "    #Perform a TimeSeriesSplit on the dataset\n",
    "    time_series_split = TimeSeriesSplit(n_splits = 3)\n",
    "\n",
    "\n",
    "    rf_cv = GridSearchCV(rf, params, cv = time_series_split, n_jobs = -1, verbose = 20)\n",
    "\n",
    "    #Fit the random forest with our X_train and Y_train\n",
    "    rf_cv.fit(X_train, Y_train)\n",
    "\n",
    "    #Save the fited variable into a Pickle file\n",
    "    file_loc = f'{os.getcwd()}\\\\Pickle_Files\\\\Cluster_{cluster_selected}'\n",
    "    pickle.dump(rf_cv, open(file_loc,'wb'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# make Prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "#Use 2nd January Data\n",
    "day_data = test_data.loc['2019-01-02']\n",
    "\n",
    "pred_for_tomorrow = pd.DataFrame({'Date':[],\n",
    "                                  'company':[],\n",
    "                                  'prediction':[]})\n",
    "\n",
    "#Predict each stock using the 2nd January Data\n",
    "for cluster_selected in clusters_df.Cluster.unique():\n",
    "    rf_cv =  pickle.load(open(os.getcwd() + f'\\\\Pickle_Files\\\\Cluster_{cluster_selected}', 'rb'))\n",
    "    best_rf = rf_cv.best_estimator_\n",
    "    cluster_data = day_data.loc[day_data.symbol.isin(clusters_df.loc[clusters_df.Cluster==cluster_selected,'Companies'].tolist())].copy()\n",
    "    cluster_data = cluster_data.dropna()\n",
    "    if (cluster_data.shape[0]>0):\n",
    "        X_test = cluster_data.loc[:,Target_variables]\n",
    "\n",
    "        pred_for_tomorrow = pred_for_tomorrow.append(pd.DataFrame({'Date':cluster_data.index,\n",
    "                                                                   'company':cluster_data['symbol'],\n",
    "                                                                   'prediction':best_rf.predict_proba(X_test)[:,1]}), ignore_index = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}