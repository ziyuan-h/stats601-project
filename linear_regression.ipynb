{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read from file and parse into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data file\n",
    "log_pr = pd.read_pickle(\"./log_price.df\")\n",
    "volume = pd.read_pickle(\"./volume_usd.df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "dt = datetime.timedelta(days=60)\n",
    "cur_t = log_pr.index[-1]\n",
    "log_pr_test = log_pr.loc[(cur_t - dt):]\n",
    "log_pr = log_pr[:(cur_t - dt)]\n",
    "volume_test = volume.loc[(cur_t - dt):]\n",
    "volume = volume[:(cur_t - dt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((86401, 10), (86401, 10), (178560, 10), (178560, 10))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_pr_test.shape, volume_test.shape, log_pr.shape, volume.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((183, 28800), (183, 10))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# divide and parse data into training set and label set\n",
    "data = pd.concat([log_pr, np.log(volume + 1)], axis=1)\n",
    "data_np = data.values\n",
    "train_np = data_np.reshape(-1, 1440 * 20)\n",
    "train_np = np.delete(train_np, -1, axis=0)\n",
    "label_np = data_np[1440+29::1440,:10]  # future log price in 30 min\n",
    "train_np.shape, label_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize each feature\n",
    "def standardize(train_np):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(train_np)\n",
    "\n",
    "train_np_standardize = standardize(train_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.0006284209797935348, 0.01311364501707388)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize each feature\n",
    "def normalize(train_np):\n",
    "    from sklearn.preprocessing import Normalizer\n",
    "\n",
    "    norm = Normalizer()\n",
    "    return norm.fit_transform(train_np)\n",
    "\n",
    "train_np_normalize = normalize(train_np)\n",
    "train_np_normalize.min(), train_np_normalize.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.30707126, 0.47496257, 0.54138948, 0.6035855 , 0.6237112 ,\n",
       "       0.63905665, 0.65100007, 0.6618374 , 0.67221408, 0.68074472,\n",
       "       0.68908342, 0.69588428, 0.70245134, 0.7083728 , 0.71418379,\n",
       "       0.7198574 , 0.72502773, 0.72995567, 0.73462558, 0.73895161,\n",
       "       0.74319612, 0.74735328, 0.75131267, 0.75499127, 0.75859269,\n",
       "       0.76201976, 0.76534245, 0.76853719, 0.77165096, 0.77470539,\n",
       "       0.77765917, 0.78052295, 0.78334896, 0.78613007, 0.78875611,\n",
       "       0.79137791, 0.79396715, 0.79648294, 0.79895304, 0.8014019 ,\n",
       "       0.80378863, 0.80613203, 0.80845572, 0.81073905, 0.81301542,\n",
       "       0.81525388, 0.81744664, 0.81959648, 0.82171636, 0.82382083,\n",
       "       0.82589077, 0.82794408, 0.8299785 , 0.83199218, 0.83396154,\n",
       "       0.83591345, 0.83785891, 0.8397871 , 0.84168492, 0.84356368,\n",
       "       0.84542494, 0.84727367, 0.84911079, 0.85092497])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract principal components\n",
    "def pca_after_normalize(train_np):\n",
    "    from sklearn.decomposition import PCA \n",
    "\n",
    "    pca = PCA(n_components=64, svd_solver=\"full\")\n",
    "    pca.fit(standardize(train_np))\n",
    "    return pca.transform(train_np), pca\n",
    "\n",
    "train_np_pca, pca = pca_after_normalize(train_np)\n",
    "np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation to train the model and return out-of-sample correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(183, 28800)\n",
      "[0.9935358183521849, 0.9927509619370319, 0.9886858325213485, 0.9928644276983495, 0.991944139851397, 0.9923916198449841, 0.9937421503207934, 0.9938208153389676, 0.9944544820070387, 0.9911745209345547]\n",
      "avg= 0.992536476880665\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import critic\n",
    "\n",
    "train_actual = train_np_normalize\n",
    "print(train_actual.shape)\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.25)\n",
    "reg = LinearRegression()\n",
    "validation_score = []\n",
    "for train_idx, test_idx in cv.split(train_actual, label_np):\n",
    "    X = np.take(train_actual, train_idx, axis=0)\n",
    "    Y = np.take(label_np, train_idx, axis=0)\n",
    "    X_test = np.take(train_actual, test_idx, axis=0)\n",
    "    Y_test = np.take(label_np, test_idx, axis=0)\n",
    "    reg.fit(X, Y)\n",
    "    \n",
    "    # calculate validation score\n",
    "    Y_hat = reg.predict(X_test)\n",
    "    score = critic.overall_corr(Y_hat, Y_test)\n",
    "    validation_score.append(score)\n",
    "\n",
    "print(validation_score)\n",
    "print(\"avg=\", np.mean(validation_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28801, 10)\n"
     ]
    }
   ],
   "source": [
    "# save regressor weights\n",
    "params = np.hstack([reg.intercept_[:,np.newaxis], reg.coef_]).T\n",
    "print(params.shape)\n",
    "np.savetxt(\"./linreg_norm.txt\", params.astype(np.single), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 28800)\n"
     ]
    }
   ],
   "source": [
    "# save pca components\n",
    "print(pca.components_.shape)\n",
    "np.savetxt(\"./pca.txt\", pca.components_.astype(np.single), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature | Preprocess | Cross Validation Score | Weights Order |\n",
    "|---------|------------|------------------------|---------------|\n",
    "| log price + volume (n_samples, 10 * 1440 * 2) | log over volumes | avg = 0.9925 | e-11 |\n",
    "| log price + volume (n_samples, 10 * 1440 * 2) | log over volumes and standardize | avg = 0.9983 | e-11 |\n",
    "| log price + volume (n_samples, 10 * 1440 * 2) | log over volumes and normalize | avg = 0.9928 | e-7 |\n",
    "| log price + volume (n_samples, 10 * 1440 * 2) | log over volumes, normalize, and pca 128 | a,vg = 0.9312 | e-6 |\n",
    "| log price + volume (n_samples, 10 * 1440 * 2) | log over volumes, standardize, and pca 64 | a,vg = 0.9976 | e-6 / e-10 for pca |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model and run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_r_hat(A, B): \n",
    "    \"\"\"\n",
    "        A: 1440-by-10 dataframe of log prices with columns log_pr_0, ... , log_pr_9\n",
    "        B: 1440-by-10 dataframe of trading volumes with columns volu_0, ... , volu_9    \n",
    "        return: a numpy array of length 10, corresponding to the predictions for the forward 30-minutes returns of assets 0, 1, 2, ..., 9\n",
    "    \"\"\"\n",
    "    \n",
    "    data = pd.concat([A, B], axis=1).values.ravel()[np.newaxis, :]\n",
    "    weights = np.loadtxt(\"./linreg.txt\", delimiter=',')\n",
    "    pred = weights[0] + (data @ weights[1:]).squeeze()\n",
    "    return A.iloc[-1].values - pred # Use the negative 30-minutes backward log-returns to predict the 30-minutes forward log-returns\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.09780466, -0.19130926,  0.44984528,  0.21735089, -0.07590324,\n",
       "       -0.40594572, -0.24346416, -0.08908863, -0.23921368, -0.27030188])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standardize + pca\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def get_r_hat(A, B):\n",
    "    \"\"\"\n",
    "        A: 1440-by-10 dataframe of log prices with columns log_pr_0, ... , log_pr_9\n",
    "        B: 1440-by-10 dataframe of trading volumes with columns volu_0, ... , volu_9    \n",
    "        return: a numpy array of length 10, corresponding to the predictions for the forward 30-minutes returns of assets 0, 1, 2, ..., 9\n",
    "    \"\"\"\n",
    "    A_np, B_np = A.values, B.values\n",
    "    data = np.concatenate([A_np, np.log(B_np)], axis=0).reshape(1, -1)\n",
    "    reg_weights = np.loadtxt(\"./linreg.txt\", delimiter=',')\n",
    "    pca_weights = np.loadtxt(\"./pca.txt\", delimiter=',')\n",
    "    data = StandardScaler().fit_transform(data)\n",
    "    data = data @ pca_weights.T\n",
    "    predict = (data @ reg_weights[1:]).squeeze() + reg_weights[0]\n",
    "    return predict\n",
    "\n",
    "get_r_hat(log_pr.iloc[-1440:], volume.iloc[-1440:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.76917557, -0.5449981 ,  1.92371925,  0.32897562,  0.01220115,\n",
       "       -0.92214199,  0.06372874, -0.1868889 ,  0.4834458 ,  0.51131822])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "def get_r_hat(A, B):\n",
    "    \"\"\"\n",
    "        A: 1440-by-10 dataframe of log prices with columns log_pr_0, ... , log_pr_9\n",
    "        B: 1440-by-10 dataframe of trading volumes with columns volu_0, ... , volu_9    \n",
    "        return: a numpy array of length 10, corresponding to the predictions for the forward 30-minutes returns of assets 0, 1, 2, ..., 9\n",
    "    \"\"\"\n",
    "    A_np, B_np = A.values, B.values\n",
    "    data = np.concatenate([A_np, np.log(B_np)], axis=0).reshape(1, -1)\n",
    "    data = Normalizer().fit_transform(data)\n",
    "    reg_weights = np.loadtxt(\"./linreg_norm.txt\", delimiter=',')\n",
    "    predict = (data @ reg_weights[1:]).squeeze() + reg_weights[0]\n",
    "    return predict\n",
    "\n",
    "get_r_hat(log_pr.iloc[-1440:], volume.iloc[-1440:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate OJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.006556\n",
       "1   -0.010191\n",
       "2   -0.006406\n",
       "3   -0.009474\n",
       "4   -0.006198\n",
       "5    0.001535\n",
       "6   -0.002478\n",
       "7   -0.006178\n",
       "8    0.000166\n",
       "9    0.004295\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import main\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "t0 = time.time()\n",
    "dt = datetime.timedelta(days=1) - datetime.timedelta(minutes=1)\n",
    "r_hat = pd.DataFrame(index=log_pr.index[1440::10], columns=np.arange(10), dtype=np.float64)\n",
    "# print(r_hat.index)\n",
    "for t in log_pr.index[1440::10]:  # compute the predictions every 10 minutes\n",
    "    # print(log_pr_test.loc[(t - dt):t].shape, volume_test.loc[(t - dt):t].shape)\n",
    "    r_hat.loc[t, :] = main.get_r_hat(log_pr.loc[(t - dt):t], volume.loc[(t - dt):t])\n",
    "t_used = time.time() - t0\n",
    "# print(t_used)\n",
    "\n",
    "r_fwd = (log_pr.shift(-30) - log_pr).iloc[1440::10].rename(columns={f\"log_pr_{i}\": i for i in range(10)})\n",
    "# print(r_fwd, r_hat)\n",
    "r_fwd.corrwith(r_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8dad618d23709a34c1193ca00eacf1e599dffaa14cca1da034597c33c3b1cb4f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
