{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0615275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "log_pr_file = './log_price.df'\n",
    "volu_usd_file = './volume_usd.df'\n",
    "\n",
    "log_pr = pd.read_pickle(log_pr_file)\n",
    "volu = pd.read_pickle(volu_usd_file)\n",
    "\n",
    "daylen = 10\n",
    "\n",
    "def interpolate(log_pr, volu, window=30):\n",
    "    log_pr.columns = ['log_pr_%d'%i for i in range(10)]\n",
    "    volu.columns = ['volu_%d'%i for i in range(10)]\n",
    "\n",
    "    open_ = log_pr[::window].reindex(log_pr.index).ffill()\n",
    "    open_.columns = ['open_%d'%i for i in range(10)]\n",
    "    close_ = log_pr[window-1::window].reindex(log_pr.index).bfill()\n",
    "    close_.columns = ['close_%d'%i for i in range(10)]\n",
    "    high_ = log_pr.groupby(np.arange(len(log_pr))//window) \\\n",
    "            .max().set_index(np.arange(0, len(log_pr), window)) \\\n",
    "            .reindex(np.arange(len(log_pr))).ffill().set_index(log_pr.index)\n",
    "    high_.columns = ['high_%d'%i for i in range(10)]\n",
    "    low_ = log_pr.groupby(np.arange(len(log_pr))//window) \\\n",
    "            .min().set_index(np.arange(0, len(log_pr), window)) \\\n",
    "            .reindex(np.arange(len(log_pr))).ffill().set_index(log_pr.index)\n",
    "    low_.columns = ['low_%d'%i for i in range(10)]\n",
    "    return pd.concat([log_pr, volu, open_, close_, high_, low_], axis=1)\n",
    "\n",
    "# data = interpolate(log_pr, volu, daylen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8fc23cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Moving Average\n",
    "def SMA(x, window):\n",
    "    return x.rolling(window).mean()\n",
    "\n",
    "# exponential moving average\n",
    "def EMA(x, window):\n",
    "    return x.ewm(com=1/window, adjust=True, min_periods=window).mean()\n",
    "\n",
    "# Average True Range\n",
    "def ATR(x, window, daylen):\n",
    "    low = x[['low_%d'%i for i in range(10)]].iloc[::daylen].copy()\n",
    "    high = x[['high_%d'%i for i in range(10)]].iloc[::daylen].copy()\n",
    "    close = x[['close_%d'%i for i in range(10)]].iloc[::daylen].copy()\n",
    "    \n",
    "    high_low = high.values - low.values\n",
    "    high_close = np.abs(high.values - close.shift().values)\n",
    "    low_close = np.abs(low.values - close.shift().values)\n",
    "\n",
    "    ranges = np.stack([high_low, high_close, low_close], axis=0)\n",
    "    true_range = np.max(ranges, axis=0)\n",
    "    true_range = pd.DataFrame(true_range, \n",
    "                              index=close.index, columns=['atr_%d'%i for i in range(10)])\n",
    "    atr = EMA(true_range, window)\n",
    "    atr = atr.reindex(x.index).ffill()\n",
    "    return atr\n",
    "\n",
    "# TODO\n",
    "# Average Directional Movement Index\n",
    "def ADX(x, window, daylen):\n",
    "    low = x[['low_%d'%i for i in range(10)]].iloc[::daylen].copy()\n",
    "    high = x[['high_%d'%i for i in range(10)]].iloc[::daylen].copy()\n",
    "    close = x[['close_%d'%i for i in range(10)]].iloc[::daylen].copy()\n",
    "    \n",
    "    plus_dm = high.diff()\n",
    "    minus_dm = low.diff()\n",
    "    plus_dm[plus_dm < 0] = 0\n",
    "    minus_dm[minus_dm > 0] = 0\n",
    "    \n",
    "    atr = ATR(x, window, daylen).iloc[::daylen]\n",
    "#     print(atr)\n",
    "    \n",
    "    plus_di = (100 * EMA(plus_dm, window) / atr.values).values\n",
    "    minus_di = abs(100 * EMA(minus_dm, window) / atr.values).values\n",
    "    \n",
    "    adx = (abs(plus_di - minus_di) / abs(plus_di + minus_di)) * 100\n",
    "    adx = pd.DataFrame(adx, index=close.index, columns=['adx_%d'%i for i in range(10)])\n",
    "    adx = ((adx.shift() * (window - 1)) + adx) / window\n",
    "    adx_smooth = EMA(adx, window)\n",
    "    adx_smooth = adx_smooth.reindex(x.index).ffill()\n",
    "    return adx_smooth\n",
    "\n",
    "# Commodity Channel Index\n",
    "def CCI(x, window, daylen):\n",
    "    low = x[['low_%d'%i for i in range(10)]].iloc[::daylen].copy()\n",
    "    high = x[['high_%d'%i for i in range(10)]].iloc[::daylen].copy()\n",
    "    close = x[['close_%d'%i for i in range(10)]].iloc[::daylen].copy()\n",
    "    \n",
    "    m = (high.values + low.values + close)/3\n",
    "#     return m\n",
    "    sma = SMA(m, window)\n",
    "#     return sma\n",
    "    mad_ = m.rolling(window).apply(lambda x: pd.Series(x).mad())\n",
    "    cci = pd.DataFrame((m.values - sma.values)/(0.015*mad_.values), \n",
    "                       index=close.index, columns=['cci_%d'%i for i in range(10)])\n",
    "    cci = cci.reindex(x.index).ffill()\n",
    "    return cci\n",
    "\n",
    "# Price Rate of Change\n",
    "def ROC(x, window, daylen):\n",
    "    close = x[['close_%d'%i for i in range(10)]].iloc[::daylen].copy()\n",
    "    roc = close.pct_change(window)\n",
    "    roc.columns = ['roc_%d'%i for i in range(10)]\n",
    "    roc = roc.reindex(x.index).ffill()\n",
    "    return roc\n",
    "\n",
    "# Relative Strength Index\n",
    "def RSI(x, window, daylen, ema=True):\n",
    "    close = x[['close_%d'%i for i in range(10)]].iloc[::daylen].copy()\n",
    "    \n",
    "    # Make two series: one for lower closes and one for higher closes\n",
    "    up = close.diff().clip(lower=0)\n",
    "    down = -1 * close.diff().clip(upper=0)\n",
    "    \n",
    "    if ema == True:\n",
    "        # Use exponential moving average\n",
    "        ma_up = EMA(up, window)\n",
    "        ma_down = EMA(down, window)\n",
    "    else:\n",
    "        # Use simple moving average\n",
    "        ma_up = SMA(up, window)\n",
    "        ma_down = SMA(down, window)\n",
    "        \n",
    "    rsi = ma_up.values / (ma_down.values + 1e-4)\n",
    "    rsi = 100 - (100/(1 + rsi))\n",
    "    rsi = pd.DataFrame(rsi, index=close.index, columns=['rsi_%d'%i for i in range(10)])\n",
    "    rsi = rsi.reindex(x.index).ffill()\n",
    "    return rsi\n",
    "\n",
    "# William's %R oscillator\n",
    "def WR(x, window):\n",
    "    hn = x[['log_pr_%d'%i for i in range(10)]].rolling(window).max()\n",
    "    ln = x[['log_pr_%d'%i for i in range(10)]].rolling(window).min()\n",
    "    wr = 100*(hn.values - x[['close_%d'%i for i in range(10)]].values)/(hn.values - ln.values)\n",
    "    return pd.DataFrame(wr, index=x.index, columns=['wr_%d'%i for i in range(10)])\n",
    "\n",
    "# Stochastic K\n",
    "def SK(x, window):\n",
    "    hhn = x[['high_%d'%i for i in range(10)]].rolling(window).max()\n",
    "    lln = x[['low_%d'%i for i in range(10)]].rolling(window).min()\n",
    "    sk = 100*(x[['close_%d'%i for i in range(10)]].values - lln.values)/(hhn.values - lln.values)\n",
    "    return pd.DataFrame(sk, index=x.index, columns=['sk_%d'%i for i in range(10)])\n",
    "\n",
    "# Stochastic D\n",
    "def SD(x, window):\n",
    "    sd = EMA(SK(x, window), 3)\n",
    "    sd.columns = ['sd_%d'%i for i in range(10)]\n",
    "    return sd\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8d8bc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature generation pipline\n",
    "def generate_features(data, window, daylen):\n",
    "    pr = data.drop(labels=['volu_%d'%i for i in range(10)], axis=1)\n",
    "    sma = SMA(pr[['log_pr_%d'%i for i in range(10)]], window)\n",
    "    sma.columns = ['sma_%d'%i for i in range(10)]\n",
    "    # print(sma.shape)\n",
    "    ema = EMA(pr[['log_pr_%d'%i for i in range(10)]], window)\n",
    "    ema.columns = ['ema_%d'%i for i in range(10)]\n",
    "    # print(ema.shape)\n",
    "    atr = ATR(pr, window, daylen)\n",
    "    # print(atr.shape)\n",
    "    adx = ADX(pr, window, daylen)\n",
    "    # print(adx.shape)\n",
    "    # cci = CCI(pr, window, daylen)\n",
    "    # print(cci.shape)\n",
    "    # roc = ROC(pr, window, daylen)\n",
    "    # print(roc.shape)\n",
    "    rsi = RSI(pr, window, daylen)\n",
    "    # print(rsi.shape)\n",
    "    wr = WR(pr, window)\n",
    "    # print(wr.shape)\n",
    "    sk = SK(pr, window)\n",
    "    # print(sk.shape)\n",
    "    sd = SD(pr, window)\n",
    "    # print(sd.shape)\n",
    "    return pd.concat([sma, ema, atr, adx, # cci, roc, \n",
    "                    rsi, wr, sk, sd], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4287aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "def data_preprocess(log_pr, volu, window, daylen, scaler_file = None):\n",
    "    data = interpolate(log_pr, volu, window)\n",
    "    features = generate_features(data, window, daylen)\n",
    "    # print(features.shape)\n",
    "    features = features.dropna()\n",
    "    # print(features.shape)\n",
    "    if isinstance(scaler_file, type(None)):\n",
    "        scaler = StandardScaler()\n",
    "        features_transform = scaler.fit_transform(features)\n",
    "        with open('scaler.pkl', 'wb') as f:\n",
    "            pickle.dump(scaler, f)\n",
    "    else:\n",
    "        with open(scaler_file, 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "        features_transform = scaler.transform(features)\n",
    "    features_transform = pd.DataFrame(features_transform, \n",
    "                                    index=features.index, \n",
    "                                    columns=features.columns)\n",
    "    return features_transform, scaler_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9456bf31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['sma_0', 'sma_1', 'sma_2', 'sma_3', 'sma_4', 'sma_5', 'sma_6', 'sma_7',\n",
       "        'sma_8', 'sma_9', 'ema_0', 'ema_1', 'ema_2', 'ema_3', 'ema_4', 'ema_5',\n",
       "        'ema_6', 'ema_7', 'ema_8', 'ema_9', 'atr_0', 'atr_1', 'atr_2', 'atr_3',\n",
       "        'atr_4', 'atr_5', 'atr_6', 'atr_7', 'atr_8', 'atr_9', 'adx_0', 'adx_1',\n",
       "        'adx_2', 'adx_3', 'adx_4', 'adx_5', 'adx_6', 'adx_7', 'adx_8', 'adx_9',\n",
       "        'rsi_0', 'rsi_1', 'rsi_2', 'rsi_3', 'rsi_4', 'rsi_5', 'rsi_6', 'rsi_7',\n",
       "        'rsi_8', 'rsi_9', 'wr_0', 'wr_1', 'wr_2', 'wr_3', 'wr_4', 'wr_5',\n",
       "        'wr_6', 'wr_7', 'wr_8', 'wr_9', 'sk_0', 'sk_1', 'sk_2', 'sk_3', 'sk_4',\n",
       "        'sk_5', 'sk_6', 'sk_7', 'sk_8', 'sk_9', 'sd_0', 'sd_1', 'sd_2', 'sd_3',\n",
       "        'sd_4', 'sd_5', 'sd_6', 'sd_7', 'sd_8', 'sd_9'],\n",
       "       dtype='object'),\n",
       " (264900, 80),\n",
       " (264900, 80))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = data_preprocess(log_pr, volu, 3, 10)[0]\n",
    "features.columns, features.shape, features.dropna().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e4cb963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('features.pkl', 'wb') as f:\n",
    "    pickle.dump(features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddd75032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('features.pkl', 'rb') as f:\n",
    "    features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6e529de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data into test set and training set\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "N = len(features)\n",
    "train_idx = np.arange(1440 * 30, 1440 * 90)\n",
    "np.random.shuffle(train_idx)\n",
    "label_idx = train_idx + 30\n",
    "test_idx = np.arange(1440*90, 1440*150)\n",
    "np.random.shuffle(test_idx)\n",
    "test_label_idx = test_idx + 30\n",
    "len(np.intersect1d(train_idx, test_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7e6ce86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((86400, 80), (86400, 10))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training set\n",
    "train_features = features.iloc[train_idx]\n",
    "train_labels = log_pr.iloc[label_idx]\n",
    "train_features.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bcfa920a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((86400, 80), (86400, 10))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validation set\n",
    "test_features = features.iloc[test_idx]\n",
    "test_labels = log_pr.iloc[test_label_idx]\n",
    "test_features.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38b8f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to train a model\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# cum_preds = dict(pred=[], true=[])\n",
    "def eval(y_pred, y_true):\n",
    "    # print(y_pred.shape, y_true.shape)\n",
    "    # cum_preds['pred'].append(y_pred.ravel())\n",
    "    # cum_preds['true'].append(y_true.ravel())\n",
    "    # preds = np.concatenate(cum_preds['pred'], axis=0)\n",
    "    # trues = np.concatenate(cum_preds['true'], axis=0)\n",
    "    return 'corr', np.corrcoef(y_true.ravel(),y_pred.ravel())[0, 1], True\n",
    "\n",
    "booster_params = dict(objective='quantile',\n",
    "                        boosting='gbdt',\n",
    "                        num_iterations=500,\n",
    "                        learning_rate=0.2,\n",
    "                        num_leaves=21,\n",
    "                        tree_learner='serial',\n",
    "                        seed=99,\n",
    "                        max_depth=2,\n",
    "                        min_data_in_leaf=100,\n",
    "                        subsample=0.2,\n",
    "                        feature_fraction=0.2,\n",
    "                        feature_fraction_bynode=0.5,\n",
    "                        feature_fraction_seed=88,\n",
    "                        early_stopping_round=10,\n",
    "                        force_row_wise=True,\n",
    "                        # force_col_wise=True,\n",
    "                        reg_alpha=0.5,\n",
    "                        reg_lambda=0.5,\n",
    "                        max_delta_step=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d86dcbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def single_model(train_features, train_labels,\n",
    "                test_features, test_labels,\n",
    "                booster_params, model_name,\n",
    "                plot=False):\n",
    "    # prepare training data\n",
    "    X = np.concatenate([train_features.iloc[:,i::10].values for i in range(10)], axis=0)\n",
    "    y = train_labels.values.T.reshape(-1, 1).ravel()\n",
    "\n",
    "    # prepare validation data\n",
    "    X_test = np.concatenate([test_features.iloc[:,i::10].values for i in range(10)], axis=0)\n",
    "    y_test = test_labels.values.T.reshape(-1, 1).ravel()\n",
    "\n",
    "    # train model or cross validation\n",
    "    print('Start training...')\n",
    "    gbm = lgb.LGBMRegressor(**booster_params)\n",
    "    history = dict()\n",
    "    callbacks = [lgb.early_stopping(10)]\n",
    "    eval_set=[(X, y), (X_test, y_test)]\n",
    "    if plot:\n",
    "        callbacks.append(lgb.record_evaluation(history))\n",
    "\n",
    "    gbm.fit(X, y, \n",
    "            eval_set=eval_set,\n",
    "            eval_metric=eval,\n",
    "                callbacks=callbacks)\n",
    "                \n",
    "    print('###########################################')\n",
    "    print('Start prediction...')\n",
    "    y_pred = gbm.predict(X_test)\n",
    "    rmse_test = mean_squared_error(y_test, y_pred)\n",
    "    print(f'The RMSE of prediction is {rmse_test}')\n",
    "    # print(gbm.evals_result_)\n",
    "    print(f'Best overall correlation of prediction is {gbm.evals_result_[\"valid_1\"][\"corr\"][-1]}')\n",
    "    print(f'Feature importances: {list(gbm.feature_importances_)}')\n",
    "    print(f'Best iteration: {gbm.best_iteration_}')\n",
    "    print(f'Best training score: {gbm.best_score_}')\n",
    "    print('###########################################')\n",
    "\n",
    "    if plot:\n",
    "        # print(history)\n",
    "        _, ax = plt.subplots(3, figsize=(12, 12))\n",
    "        lgb.plot_importance(gbm, ax=ax[0])\n",
    "        lgb.plot_split_value_histogram(gbm, 0, ax=ax[1])\n",
    "        ax[2].plot(history['training']['corr'], 'g-', label='training corr')\n",
    "        ax[2].plot(history['valid_1']['corr'], 'b-', label='eval corr')\n",
    "        ax[2].legend()\n",
    "        ax[2].grid()\n",
    "        plt.show()\n",
    "\n",
    "    with open(model_name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(gbm, f)\n",
    "\n",
    "    return gbm    \n",
    "\n",
    "# train all assets with the separate model\n",
    "def separate_model(train_features, train_labels, \n",
    "            test_features, test_labels,\n",
    "            booster_params, model_name,\n",
    "            plot=False,\n",
    "            cv=False, cv_params=None):\n",
    "    # prepare training data\n",
    "    X = np.concatenate([train_features.iloc[:,i::10].values for i in range(10)], axis=0)\n",
    "    y = train_labels.values.T.reshape(-1, 1).ravel()\n",
    "\n",
    "    # prepare validation data\n",
    "    X_test = np.concatenate([test_features.iloc[:,i::10].values for i in range(10)], axis=0)\n",
    "    y_test = test_labels.values.T.reshape(-1, 1).ravel()\n",
    "\n",
    "    # train model or cross validation\n",
    "    print('Start training...')\n",
    "    gbm = lgb.LGBMRegressor(**booster_params)\n",
    "    history = dict()\n",
    "    callbacks = [lgb.early_stopping(10)]\n",
    "    eval_set=[(X, y), (X_test, y_test)]\n",
    "    if plot:\n",
    "        callbacks.append(lgb.record_evaluation(history))\n",
    "\n",
    "    if not cv:\n",
    "        gbm.fit(X, y, \n",
    "                eval_set=eval_set,\n",
    "                eval_metric=eval,\n",
    "                callbacks=callbacks)\n",
    "    else:\n",
    "        cvm = RandomizedSearchCV(gbm, **cv_params)\n",
    "        cvm.fit(X, y.ravel(), \n",
    "                eval_set=eval_set,\n",
    "                eval_metric=eval,\n",
    "                callbacks=callbacks)\n",
    "        gbm = cvm.best_estimator_\n",
    "\n",
    "    print('###########################################')\n",
    "    print('Start prediction...')\n",
    "    y_pred = gbm.predict(X_test)\n",
    "    rmse_test = mean_squared_error(y_test, y_pred)\n",
    "    print(f'The RMSE of prediction is {rmse_test}')\n",
    "    # print(gbm.evals_result_)\n",
    "    print(f'Best overall correlation of prediction is {gbm.evals_result_[\"valid_1\"][\"corr\"][-1]}')\n",
    "    print(f'Feature importances: {list(gbm.feature_importances_)}')\n",
    "    print(f'Best iteration: {gbm.best_iteration_}')\n",
    "    print(f'Best training score: {gbm.best_score_}')\n",
    "    print('###########################################')\n",
    "\n",
    "    if plot:\n",
    "        # print(history)\n",
    "        _, ax = plt.subplots(3, figsize=(12, 12))\n",
    "        lgb.plot_importance(gbm, ax=ax[0])\n",
    "        lgb.plot_split_value_histogram(gbm, 0, ax=ax[1])\n",
    "        ax[2].plot(history['training']['corr'], 'g-', label='training corr')\n",
    "        ax[2].plot(history['valid_1']['corr'], 'b-', label='eval corr')\n",
    "        ax[2].legend()\n",
    "        ax[2].grid()\n",
    "        plt.show()\n",
    "\n",
    "    with open(model_name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(gbm, f)\n",
    "\n",
    "    return gbm\n",
    "    \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e330cf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "distributions = dict(objective=['quantile', 'tweedie', 'gamma', \n",
    "                                'l2', 'l1', 'huber', 'fair', 'mape'],\n",
    "                        boosting=['gbdt', 'rf', 'dart', 'goss'],\n",
    "                        # num_iterations=np.random.randint(10, 200, 20),\n",
    "                        learning_rate=uniform(loc=0, scale=1),\n",
    "                        num_leaves=np.random.randint(2, 31, 10),\n",
    "                        tree_learner=['serial', 'feature', 'data'],\n",
    "                        seed=[99],\n",
    "                        max_depth=np.random.randint(2, 10, 10),\n",
    "                        min_data_in_leaf=np.random.randint(10, 100, 10),\n",
    "                        subsample=uniform(loc=0, scale=0.6),\n",
    "                        feature_fraction=uniform(loc=0, scale=1),\n",
    "                        feature_fraction_bynode=uniform(loc=0, scale=1),\n",
    "                        feature_fraction_seed=[88],\n",
    "                        early_stopping_round=np.random.randint(5, 20, 1000),\n",
    "                        force_row_wise=[True],\n",
    "                        # force_col_wise=True,\n",
    "                        reg_alpha=uniform(loc=0, scale=1),\n",
    "                        reg_lambda=uniform(loc=0, scale=1),\n",
    "                        max_delta_step=uniform(loc=0, scale=1),\n",
    "                        verbose=[-1])\n",
    "                        \n",
    "cv_params = dict(param_distributions=distributions,\n",
    "                n_iter=10,\n",
    "                refit=True,\n",
    "                cv=20,\n",
    "                verbose=-1,\n",
    "                random_state=78,\n",
    "                return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0aead5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# train all assets with the same model\n",
    "def all_same_model(train_features, train_labels, \n",
    "            test_features, test_labels,\n",
    "            booster_params, model_name,\n",
    "            plot=False,\n",
    "            cv=False, cv_params=None):\n",
    "    # prepare training data\n",
    "    X = train_features.values\n",
    "    y = train_labels.values.ravel()\n",
    "\n",
    "    # prepare validation data\n",
    "    X_test = test_features.values\n",
    "    y_test = test_labels.values.ravel()\n",
    "\n",
    "    # train model or cross validation\n",
    "    print('Start training...')\n",
    "    gbm = lgb.LGBMRegressor(**booster_params)\n",
    "    history = dict()\n",
    "    callbacks = [lgb.early_stopping(10)]\n",
    "    eval_set=[(X, y), (X_test, y_test)]\n",
    "    if plot:\n",
    "        callbacks.append(lgb.record_evaluation(history))\n",
    "\n",
    "    if not cv:\n",
    "        gbm.fit(X, y, \n",
    "                eval_set=eval_set,\n",
    "                eval_metric=eval,\n",
    "                callbacks=callbacks)\n",
    "    else:\n",
    "        cvm = RandomizedSearchCV(gbm, **cv_params)\n",
    "        cvm.fit(X, y.ravel(), \n",
    "                eval_set=eval_set,\n",
    "                eval_metric=eval,\n",
    "                callbacks=callbacks)\n",
    "        gbm = cvm.best_estimator_\n",
    "\n",
    "    print('###########################################')\n",
    "    print('Start prediction...')\n",
    "    y_pred = gbm.predict(X_test)\n",
    "    rmse_test = mean_squared_error(y_test, y_pred)\n",
    "    print(f'The RMSE of prediction is {rmse_test}')\n",
    "    # print(gbm.evals_result_)\n",
    "    print(f'Best overall correlation of prediction is {gbm.evals_result_[\"valid_1\"][\"corr\"][-1]}')\n",
    "    print(f'Feature importances: {list(gbm.feature_importances_)}')\n",
    "    print(f'Best iteration: {gbm.best_iteration_}')\n",
    "    print(f'Best training score: {gbm.best_score_}')\n",
    "    print('###########################################')\n",
    "\n",
    "    if plot:\n",
    "        # print(history)\n",
    "        _, ax = plt.subplots(3, figsize=(12, 12))\n",
    "        lgb.plot_importance(gbm, ax=ax[0])\n",
    "        lgb.plot_split_value_histogram(gbm, 0, ax=ax[1])\n",
    "        ax[2].plot(history['training']['corr'], 'g-', label='training corr')\n",
    "        ax[2].plot(history['valid_1']['corr'], 'b-', label='eval corr')\n",
    "        ax[2].legend()\n",
    "        ax[2].grid()\n",
    "        plt.show()\n",
    "\n",
    "    with open(model_name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(gbm, f)\n",
    "\n",
    "    return gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d5f07c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86400, 80)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac2b4388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = all_same_model(train_features, train_labels, \n",
    "#             test_features, test_labels, \n",
    "#             booster_params, 'asset0', plot=True, cv=True, cv_params=cv_params)\n",
    "from joblib import Parallel, delayed\n",
    "model = Parallel(n_jobs=-1)(delayed(all_same_model)(\n",
    "            train_features.iloc[:,i::10], train_labels.iloc[:,i], \n",
    "            test_features.iloc[:,i::10], test_labels.iloc[:,i], \n",
    "            booster_params, f'asset{i}', plot=False, cv=True, cv_params=cv_params) \n",
    "            for i in range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ac25752",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_separate.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fc5eba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "with open('model_separate.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "968c902e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8496/8496 [06:28<00:00, 21.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time used: 388.152s\n",
      "Pairwise correlation:\n",
      "\tasset 0 = 0.00764\n",
      "\tasset 1 = 0.04512\n",
      "\tasset 2 = 0.04589\n",
      "\tasset 3 = 0.02112\n",
      "\tasset 4 = 0.03777\n",
      "\tasset 5 = 0.02192\n",
      "\tasset 6 = 0.03350\n",
      "\tasset 7 = 0.02367\n",
      "\tasset 8 = 0.04987\n",
      "\tasset 9 = 0.03333\n",
      "\tmean correlation = 0.03198\n",
      "Overall correlation: -0.00366\n",
      "===============================\n",
      "Fail to outperform Ziwei's method, whose pairwise average\n",
      "and overall correlations are (0.02840, 0.01536)\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(388.152113199234,\n",
       " 0    0.007642\n",
       " 1    0.045116\n",
       " 2    0.045886\n",
       " 3    0.021121\n",
       " 4    0.037772\n",
       " 5    0.021916\n",
       " 6    0.033498\n",
       " 7    0.023665\n",
       " 8    0.049867\n",
       " 9    0.033332\n",
       " dtype: float64,\n",
       " -0.0036587059251268353)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check oj simulator's result\n",
    "def get_r_hat(A, B):\n",
    "    features = data_preprocess(A, B, 30, 10, scaler_file='scaler.pkl')[0]\n",
    "    pred = np.zeros(10)\n",
    "    for i in range(10):\n",
    "        pred[i] = model[i].predict(features.values[[-1],i::10])\n",
    "    return pred - A.values[-1]\n",
    "\n",
    "import critic\n",
    "cr = critic.Critic()\n",
    "cr.submit(get_r_hat, log_pr.iloc[-1440*60:], volu.iloc[-1440*60:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "342de647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86400, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88d24a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((264888, 100), (264888, 10))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape, features.values[:,0::10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad08e8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from  sklearn.ensemble import GradientBoostingRegressor\n",
    "from pickle import dump, load\n",
    "def train(i):\n",
    "    reg = GradientBoostingRegressor().fit(train_features.values[:,i::10],train_labels.values[:,i])\n",
    "    return reg\n",
    "XGB = Parallel(n_jobs=-1)(delayed(train)(i) for i in range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "144d37ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(XGB, open(\"XGB\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5fe359be",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = load(open(\"XGB\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "56c17f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8496/8496 [07:07<00:00, 19.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time used: 427.660s\n",
      "Pairwise correlation:\n",
      "\tasset 0 = 0.00994\n",
      "\tasset 1 = 0.04548\n",
      "\tasset 2 = 0.04764\n",
      "\tasset 3 = 0.02215\n",
      "\tasset 4 = 0.05501\n",
      "\tasset 5 = 0.02166\n",
      "\tasset 6 = 0.03079\n",
      "\tasset 7 = 0.02538\n",
      "\tasset 8 = 0.06223\n",
      "\tasset 9 = 0.03557\n",
      "\tmean correlation = 0.03558\n",
      "Overall correlation: 0.00059\n",
      "===============================\n",
      "Fail to outperform Ziwei's method, whose pairwise average\n",
      "and overall correlations are (0.02840, 0.01536)\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(427.660462141037,\n",
       " 0    0.009942\n",
       " 1    0.045484\n",
       " 2    0.047643\n",
       " 3    0.022147\n",
       " 4    0.055005\n",
       " 5    0.021662\n",
       " 6    0.030788\n",
       " 7    0.025377\n",
       " 8    0.062228\n",
       " 9    0.035568\n",
       " dtype: float64,\n",
       " 0.0005909925001908479)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_r_hat(A,B):\n",
    "    features = data_preprocess(A, B, 30, 10, scaler_file='scaler.pkl')[0].values[[-1]]\n",
    "    #print(features.shape)\n",
    "    pred = np.array([H[i].predict(features[:,i::10]) for i in range(10)])\n",
    "    #print(pred.shape,A.values.T[:,-1].shape)\n",
    "    return (pred[:,0] - A.values.T[:,-1])#*np.square([3.4842,8.5792,5.1055,0.7030,4.9008,8.2304,3.8177,2.0604,1.1499,8.3473])\n",
    " \n",
    " \n",
    "import ojsim\n",
    "sim = ojsim.OJSimulator()\n",
    "sim.submit(get_r_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2cc8d52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 26/8496 [00:01<07:37, 18.53it/s]"
     ]
    }
   ],
   "source": [
    "import main\n",
    "import ojsim\n",
    "ojsim.OJSimulator().submit(main.get_r_hat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
