{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## STATS 601 Project workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Import Libraries\n",
    "Note that pandas version has to be 1.4.x or higher and python version has to be 3.8.x or higher in order to read the pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.22.1\n",
      "  Downloading numpy-1.22.1-cp310-cp310-win_amd64.whl (14.7 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.22.1\n",
      "Collecting pandas==1.4.1\n",
      "  Downloading pandas-1.4.1-cp310-cp310-win_amd64.whl (10.6 MB)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\vapps\\anaconda3\\envs\\stats601\\lib\\site-packages (from pandas==1.4.1) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\vapps\\anaconda3\\envs\\stats601\\lib\\site-packages (from pandas==1.4.1) (1.22.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\vapps\\anaconda3\\envs\\stats601\\lib\\site-packages (from python-dateutil>=2.8.1->pandas==1.4.1) (1.16.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.4.1 pytz-2022.1\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.5.1-cp310-cp310-win_amd64.whl (7.2 MB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.2-cp310-cp310-win_amd64.whl (55 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.31.2-py3-none-any.whl (899 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\vapps\\anaconda3\\envs\\stats601\\lib\\site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\vapps\\anaconda3\\envs\\stats601\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\vapps\\anaconda3\\envs\\stats601\\lib\\site-packages (from matplotlib) (1.22.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\vapps\\anaconda3\\envs\\stats601\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-9.1.0-cp310-cp310-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\vapps\\anaconda3\\envs\\stats601\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: pillow, kiwisolver, fonttools, cycler, matplotlib\n",
      "Successfully installed cycler-0.11.0 fonttools-4.31.2 kiwisolver-1.4.2 matplotlib-3.5.1 pillow-9.1.0\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.0.2-cp310-cp310-win_amd64.whl (7.2 MB)\n",
      "Collecting scipy>=1.1.0\n",
      "  Downloading scipy-1.8.0-cp310-cp310-win_amd64.whl (37.0 MB)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\vapps\\anaconda3\\envs\\stats601\\lib\\site-packages (from scikit-learn->sklearn) (1.22.1)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=c4ad60d865459249f9be607f052aac25505d1a2c663de883df1d7b502be74492\n",
      "  Stored in directory: \\\\engin-labs.m.storage.umich.edu\\tywwyt\\windat.v2\\appdata\\local\\pip\\cache\\wheels\\9b\\13\\01\\6f3a7fd641f90e1f6c8c7cded057f3394f451f340371c68f3d\n",
      "Successfully built sklearn\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.1.0 scikit-learn-1.0.2 scipy-1.8.0 sklearn-0.0 threadpoolctl-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install numpy==1.22.1\n",
    "!pip3 install pandas==1.4.1\n",
    "!pip3 install matplotlib\n",
    "!pip3 install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# !pip3 install bayesian-optimization\n",
    "# import hyperparam\n",
    "import critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17709, 2, 1440, 10), (17709, 10))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ojsim\n",
    "sim = ojsim.OJSimulator()\n",
    "X,y = sim.formulized_train\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def AllFeatureExtract(A):\n",
    "    A0_pr = A[:,:1440]\n",
    "    A0_vo = A[:,1440:]+1\n",
    "    feature  = A0_pr[:,-1] - A0_pr[:,0]\n",
    "    #VO volumn log\n",
    "    feature = np.concatenate((feature[:,None], np.log(A0_vo)[:,-30:]), axis = 1)\n",
    "    \n",
    "    #VO difference vol (#sample, 31:60)\n",
    "    diff_vol_step = 30\n",
    "    diff_vol = A0_vo[:,diff_vol_step:] - A0_vo[:,:-diff_vol_step]\n",
    "    feature = np.append(feature, diff_vol[:,-30:], axis = 1)\n",
    "    \n",
    "    #VO rate of change (#sample, 61:90)\n",
    "    df_vo = pd.DataFrame(A0_vo.T)\n",
    "    pct_chg_fxn = lambda x: x.pct_change()\n",
    "    feature = np.append(feature,df_vo.apply(pct_chg_fxn).T.to_numpy()[:,-30:], axis = 1)\n",
    "    \n",
    "    #VO moving avg (#sample,91:120)\n",
    "    avg_step = 30\n",
    "    ma_30 = lambda x: x.rolling(avg_step).mean()\n",
    "    df_vo.apply(ma_30).apply(np.log).T.to_numpy()[:,avg_step-1:]\n",
    "    feature = np.append(feature,df_vo.apply(ma_30).apply(np.log).T.to_numpy()[:,-30:], axis = 1)\n",
    "    \n",
    "    #PR z_score (#sample, 121:150)\n",
    "    z_score_min_period = 20\n",
    "    df_pr = pd.DataFrame(A0_pr)\n",
    "    zscore_fxn = lambda x: (x - x.mean()) / x.std()\n",
    "    zscore_fun_improved =lambda x: (x - x.rolling(window=200, min_periods=z_score_min_period).mean())/ x.rolling(window=200, min_periods=z_score_min_period).std()\n",
    "    df_pr.T.apply(zscore_fun_improved).T.to_numpy()[:,z_score_min_period-1:]\n",
    "    feature = np.append(feature,df_pr.T.apply(zscore_fun_improved).T.to_numpy()[:,-30:], axis = 1)\n",
    "    \n",
    "    #VO binning (#sample, 151:180)\n",
    "    n_bins = 10\n",
    "    bin_fxn = lambda y: pd.qcut(y,q=n_bins,labels = range(1,n_bins+1))\n",
    "    binning = df_vo.apply(bin_fxn).T\n",
    "    feature = np.append(feature,binning.to_numpy()[:,-30:], axis = 1)\n",
    "    \n",
    "    #VO sign (#sample, 181:210)\n",
    "    feature = np.append(feature,(df_vo.apply(pct_chg_fxn).apply(np.sign).to_numpy().T)[:,-30:], axis = 1)\n",
    "    \n",
    "    #VO plus-minus (#sample, 211:240)\n",
    "    plus_minus_fxn = lambda x: x.rolling(20).sum()\n",
    "    (df_vo.apply(pct_chg_fxn).apply(np.sign).to_numpy()[-30:,:]).T.shape\n",
    "    feature = np.append(feature,(df_vo.apply(pct_chg_fxn).apply(np.sign).to_numpy()[-30:,:]).T, axis = 1)\n",
    "    \n",
    "    #PR difference (#sample, 241:270)\n",
    "    diff_pr_step = 30\n",
    "    diff_pr = A0_pr[:,diff_vol_step:] - A0_pr[:,:-diff_vol_step]\n",
    "    #print(\"PR difference\", A0_pr[:,diff_vol_step:] - A0_pr[:,:-diff_vol_step])\n",
    "    feature = np.append(feature, diff_pr[:,-30:], axis = 1)\n",
    "    \n",
    "    #PR rate of change (#sample, 271:300)\n",
    "    df_pr = pd.DataFrame(A0_pr.T)\n",
    "    pct_chg_fxn = lambda x: x.pct_change()\n",
    "    #print(\"PR rate of change\", df_pr.apply(pct_chg_fxn).T.to_numpy()[:,-30:])\n",
    "    feature = np.append(feature,df_pr.apply(pct_chg_fxn).T.to_numpy()[:,-30:], axis = 1)\n",
    "    \n",
    "    #PR moving avg (#sample,301:330)\n",
    "    avg_step = 30\n",
    "    ma_30 = lambda x: x.rolling(avg_step).mean()\n",
    "    df_pr.apply(ma_30).T.to_numpy()[:,avg_step-1:]\n",
    "    #print(\"PR moving avg\", df_pr.apply(ma_30).T.to_numpy()[:,-30:])\n",
    "    feature = np.append(feature,df_pr.apply(ma_30).T.to_numpy()[:,-30:], axis = 1) \n",
    "    \n",
    "    #PR binning (#sample, 331:360)\n",
    "    n_bins = 10\n",
    "    bin_fxn = lambda y: pd.qcut(y,q=n_bins,labels = range(1,n_bins+1))\n",
    "    binning = df_pr.apply(bin_fxn).T\n",
    "    #print(\"PR binning\", binning.to_numpy()[:,-30:])\n",
    "    feature = np.append(feature,binning.to_numpy()[:,-30:], axis = 1)\n",
    "    \n",
    "    #PR sign (#sample, 361:390)\n",
    "    #print(\"PR sign\", (df_pr.apply(pct_chg_fxn).apply(np.sign).to_numpy().T)[:,-30:])\n",
    "    feature = np.append(feature,(df_pr.apply(pct_chg_fxn).apply(np.sign).to_numpy().T)[:,-30:], axis = 1)\n",
    "    \n",
    "    #PR plus-minus (#sample, 391:420)\n",
    "    plus_minus_fxn = lambda x: x.rolling(20).sum()\n",
    "    #print(\"PR plus-minus\", (df_pr.apply(pct_chg_fxn).apply(np.sign).to_numpy()[-30:,:]).T)\n",
    "    feature = np.append(feature,(df_pr.apply(pct_chg_fxn).apply(np.sign).to_numpy()[-30:,:]).T, axis = 1)\n",
    "    \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "asset_num = 7\n",
    "Y_train = y[:,asset_num]\n",
    "input = np.concatenate((X[:,0,:,asset_num], X[:,1,:,asset_num]), axis = 1)  \n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train = AllFeatureExtract(input)\n",
    "Y_train = y[:,asset_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0027591163082453427 12.38569718050323 12.633329656503417 ... 1.0 1.0\n",
      "  -1.0]\n",
      " [0.004403154955420034 10.43892993469856 11.918278256237759 ... 1.0 1.0\n",
      "  -1.0]\n",
      " [0.00478942136194767 12.488029207257481 12.003846125088653 ... -1.0 1.0\n",
      "  1.0]\n",
      " ...\n",
      " [0.035322901580066984 12.19656309584971 12.556592092806685 ... -1.0 1.0\n",
      "  1.0]\n",
      " [0.03475093625565484 11.69516911381277 11.620290794584896 ... 1.0 -1.0\n",
      "  -1.0]\n",
      " [0.03800958496170853 11.658930061629839 11.17112007310433 ... 1.0 -1.0\n",
      "  -1.0]] SelectKBest(k='all',\n",
      "            score_func=<function mutual_info_regression at 0x000001720BCEB8B0>)\n",
      "Feature 0: 0.440647\n",
      "Feature 1: 0.076885\n",
      "Feature 2: 0.064966\n",
      "Feature 3: 0.071145\n",
      "Feature 4: 0.082040\n",
      "Feature 5: 0.078728\n",
      "Feature 6: 0.081110\n",
      "Feature 7: 0.085514\n",
      "Feature 8: 0.083609\n",
      "Feature 9: 0.070558\n",
      "Feature 10: 0.080422\n",
      "Feature 11: 0.076250\n",
      "Feature 12: 0.062803\n",
      "Feature 13: 0.076637\n",
      "Feature 14: 0.080356\n",
      "Feature 15: 0.065860\n",
      "Feature 16: 0.085110\n",
      "Feature 17: 0.079927\n",
      "Feature 18: 0.083791\n",
      "Feature 19: 0.080907\n",
      "Feature 20: 0.073797\n",
      "Feature 21: 0.077291\n",
      "Feature 22: 0.062550\n",
      "Feature 23: 0.080284\n",
      "Feature 24: 0.071043\n",
      "Feature 25: 0.078502\n",
      "Feature 26: 0.076825\n",
      "Feature 27: 0.082780\n",
      "Feature 28: 0.082598\n",
      "Feature 29: 0.070968\n",
      "Feature 30: 0.077345\n",
      "Feature 31: 0.028034\n",
      "Feature 32: 0.009457\n",
      "Feature 33: 0.017588\n",
      "Feature 34: 0.023494\n",
      "Feature 35: 0.031321\n",
      "Feature 36: 0.015012\n",
      "Feature 37: 0.026018\n",
      "Feature 38: 0.017941\n",
      "Feature 39: 0.028332\n",
      "Feature 40: 0.022308\n",
      "Feature 41: 0.030493\n",
      "Feature 42: 0.022178\n",
      "Feature 43: 0.016767\n",
      "Feature 44: 0.021765\n",
      "Feature 45: 0.035155\n",
      "Feature 46: 0.020897\n",
      "Feature 47: 0.025771\n",
      "Feature 48: 0.019754\n",
      "Feature 49: 0.027283\n",
      "Feature 50: 0.022073\n",
      "Feature 51: 0.021753\n",
      "Feature 52: 0.018544\n",
      "Feature 53: 0.023022\n",
      "Feature 54: 0.020397\n",
      "Feature 55: 0.017626\n",
      "Feature 56: 0.022490\n",
      "Feature 57: 0.027085\n",
      "Feature 58: 0.015004\n",
      "Feature 59: 0.030383\n",
      "Feature 60: 0.028369\n",
      "Feature 61: 0.004216\n",
      "Feature 62: 0.003097\n",
      "Feature 63: 0.011303\n",
      "Feature 64: 0.000000\n",
      "Feature 65: 0.006523\n",
      "Feature 66: 0.009208\n",
      "Feature 67: 0.004755\n",
      "Feature 68: 0.004235\n",
      "Feature 69: 0.000000\n",
      "Feature 70: 0.000000\n",
      "Feature 71: 0.006734\n",
      "Feature 72: 0.009922\n",
      "Feature 73: 0.012681\n",
      "Feature 74: 0.002462\n",
      "Feature 75: 0.000000\n",
      "Feature 76: 0.007737\n",
      "Feature 77: 0.006887\n",
      "Feature 78: 0.003841\n",
      "Feature 79: 0.000026\n",
      "Feature 80: 0.002968\n",
      "Feature 81: 0.001946\n",
      "Feature 82: 0.005352\n",
      "Feature 83: 0.007830\n",
      "Feature 84: 0.000000\n",
      "Feature 85: 0.002774\n",
      "Feature 86: 0.006247\n",
      "Feature 87: 0.000000\n",
      "Feature 88: 0.002498\n",
      "Feature 89: 0.000000\n",
      "Feature 90: 0.000000\n",
      "Feature 91: 0.188113\n",
      "Feature 92: 0.183659\n",
      "Feature 93: 0.187497\n",
      "Feature 94: 0.184903\n",
      "Feature 95: 0.175812\n",
      "Feature 96: 0.188094\n",
      "Feature 97: 0.183576\n",
      "Feature 98: 0.178696\n",
      "Feature 99: 0.174323\n",
      "Feature 100: 0.184409\n",
      "Feature 101: 0.188530\n",
      "Feature 102: 0.181402\n",
      "Feature 103: 0.190950\n",
      "Feature 104: 0.187631\n",
      "Feature 105: 0.183523\n",
      "Feature 106: 0.185300\n",
      "Feature 107: 0.182658\n",
      "Feature 108: 0.173469\n",
      "Feature 109: 0.189377\n",
      "Feature 110: 0.183723\n",
      "Feature 111: 0.191092\n",
      "Feature 112: 0.179019\n",
      "Feature 113: 0.186977\n",
      "Feature 114: 0.186897\n",
      "Feature 115: 0.184859\n",
      "Feature 116: 0.191425\n",
      "Feature 117: 0.192133\n",
      "Feature 118: 0.180271\n",
      "Feature 119: 0.194436\n",
      "Feature 120: 0.187757\n",
      "Feature 121: 0.018221\n",
      "Feature 122: 0.030295\n",
      "Feature 123: 0.038356\n",
      "Feature 124: 0.022701\n",
      "Feature 125: 0.016373\n",
      "Feature 126: 0.030745\n",
      "Feature 127: 0.029834\n",
      "Feature 128: 0.029657\n",
      "Feature 129: 0.020224\n",
      "Feature 130: 0.027240\n",
      "Feature 131: 0.007048\n",
      "Feature 132: 0.026313\n",
      "Feature 133: 0.032359\n",
      "Feature 134: 0.025356\n",
      "Feature 135: 0.030934\n",
      "Feature 136: 0.021742\n",
      "Feature 137: 0.027693\n",
      "Feature 138: 0.023216\n",
      "Feature 139: 0.021817\n",
      "Feature 140: 0.022060\n",
      "Feature 141: 0.013755\n",
      "Feature 142: 0.023413\n",
      "Feature 143: 0.028967\n",
      "Feature 144: 0.019452\n",
      "Feature 145: 0.022155\n",
      "Feature 146: 0.021537\n",
      "Feature 147: 0.021056\n",
      "Feature 148: 0.024639\n",
      "Feature 149: 0.029439\n",
      "Feature 150: 0.027305\n",
      "Feature 151: 0.003138\n",
      "Feature 152: 0.008647\n",
      "Feature 153: 0.004377\n",
      "Feature 154: 0.002845\n",
      "Feature 155: 0.008169\n",
      "Feature 156: 0.004441\n",
      "Feature 157: 0.008621\n",
      "Feature 158: 0.000000\n",
      "Feature 159: 0.009038\n",
      "Feature 160: 0.008445\n",
      "Feature 161: 0.007160\n",
      "Feature 162: 0.005588\n",
      "Feature 163: 0.000000\n",
      "Feature 164: 0.006381\n",
      "Feature 165: 0.006412\n",
      "Feature 166: 0.012367\n",
      "Feature 167: 0.004188\n",
      "Feature 168: 0.006117\n",
      "Feature 169: 0.004360\n",
      "Feature 170: 0.006883\n",
      "Feature 171: 0.008216\n",
      "Feature 172: 0.005297\n",
      "Feature 173: 0.005037\n",
      "Feature 174: 0.006271\n",
      "Feature 175: 0.000000\n",
      "Feature 176: 0.000218\n",
      "Feature 177: 0.005232\n",
      "Feature 178: 0.011390\n",
      "Feature 179: 0.018453\n",
      "Feature 180: 0.011839\n",
      "Feature 181: 0.000000\n",
      "Feature 182: 0.002490\n",
      "Feature 183: 0.000000\n",
      "Feature 184: 0.000000\n",
      "Feature 185: 0.000876\n",
      "Feature 186: 0.000000\n",
      "Feature 187: 0.000677\n",
      "Feature 188: 0.000000\n",
      "Feature 189: 0.001573\n",
      "Feature 190: 0.000000\n",
      "Feature 191: 0.000913\n",
      "Feature 192: 0.000000\n",
      "Feature 193: 0.005395\n",
      "Feature 194: 0.011351\n",
      "Feature 195: 0.003443\n",
      "Feature 196: 0.000000\n",
      "Feature 197: 0.000980\n",
      "Feature 198: 0.001373\n",
      "Feature 199: 0.000000\n",
      "Feature 200: 0.000000\n",
      "Feature 201: 0.002730\n",
      "Feature 202: 0.004670\n",
      "Feature 203: 0.004705\n",
      "Feature 204: 0.000000\n",
      "Feature 205: 0.001733\n",
      "Feature 206: 0.003833\n",
      "Feature 207: 0.003955\n",
      "Feature 208: 0.002841\n",
      "Feature 209: 0.001577\n",
      "Feature 210: 0.004294\n",
      "Feature 211: 0.000000\n",
      "Feature 212: 0.002490\n",
      "Feature 213: 0.000000\n",
      "Feature 214: 0.000000\n",
      "Feature 215: 0.000876\n",
      "Feature 216: 0.000000\n",
      "Feature 217: 0.000677\n",
      "Feature 218: 0.000000\n",
      "Feature 219: 0.001573\n",
      "Feature 220: 0.000000\n",
      "Feature 221: 0.000913\n",
      "Feature 222: 0.000000\n",
      "Feature 223: 0.005395\n",
      "Feature 224: 0.011351\n",
      "Feature 225: 0.003443\n",
      "Feature 226: 0.000000\n",
      "Feature 227: 0.000980\n",
      "Feature 228: 0.001373\n",
      "Feature 229: 0.000000\n",
      "Feature 230: 0.000000\n",
      "Feature 231: 0.002730\n",
      "Feature 232: 0.004670\n",
      "Feature 233: 0.004705\n",
      "Feature 234: 0.000000\n",
      "Feature 235: 0.001733\n",
      "Feature 236: 0.003833\n",
      "Feature 237: 0.003955\n",
      "Feature 238: 0.002841\n",
      "Feature 239: 0.001577\n",
      "Feature 240: 0.004294\n",
      "Feature 241: 0.034068\n",
      "Feature 242: 0.032816\n",
      "Feature 243: 0.037349\n",
      "Feature 244: 0.028277\n",
      "Feature 245: 0.041136\n",
      "Feature 246: 0.030795\n",
      "Feature 247: 0.022544\n",
      "Feature 248: 0.036120\n",
      "Feature 249: 0.032137\n",
      "Feature 250: 0.031106\n",
      "Feature 251: 0.037049\n",
      "Feature 252: 0.027454\n",
      "Feature 253: 0.024947\n",
      "Feature 254: 0.039983\n",
      "Feature 255: 0.027129\n",
      "Feature 256: 0.033990\n",
      "Feature 257: 0.027440\n",
      "Feature 258: 0.032722\n",
      "Feature 259: 0.031160\n",
      "Feature 260: 0.022972\n",
      "Feature 261: 0.027096\n",
      "Feature 262: 0.027010\n",
      "Feature 263: 0.019830\n",
      "Feature 264: 0.026812\n",
      "Feature 265: 0.031263\n",
      "Feature 266: 0.029696\n",
      "Feature 267: 0.029161\n",
      "Feature 268: 0.035494\n",
      "Feature 269: 0.030148\n",
      "Feature 270: 0.030908\n",
      "Feature 271: 0.443959\n",
      "Feature 272: 0.452239\n",
      "Feature 273: 0.428116\n",
      "Feature 274: 0.435494\n",
      "Feature 275: 0.439726\n",
      "Feature 276: 0.433764\n",
      "Feature 277: 0.445402\n",
      "Feature 278: 0.446036\n",
      "Feature 279: 0.439699\n",
      "Feature 280: 0.416899\n",
      "Feature 281: 0.445325\n",
      "Feature 282: 0.451633\n",
      "Feature 283: 0.435096\n",
      "Feature 284: 0.439240\n",
      "Feature 285: 0.439573\n",
      "Feature 286: 0.436660\n",
      "Feature 287: 0.449627\n",
      "Feature 288: 0.446281\n",
      "Feature 289: 0.437341\n",
      "Feature 290: 0.440651\n",
      "Feature 291: 0.448047\n",
      "Feature 292: 0.455343\n",
      "Feature 293: 0.435976\n",
      "Feature 294: 0.447332\n",
      "Feature 295: 0.439840\n",
      "Feature 296: 0.437365\n",
      "Feature 297: 0.451287\n",
      "Feature 298: 0.440919\n",
      "Feature 299: 0.437964\n",
      "Feature 300: 0.432724\n",
      "Feature 301: 2.871318\n",
      "Feature 302: 2.878954\n",
      "Feature 303: 2.886488\n",
      "Feature 304: 2.895425\n",
      "Feature 305: 2.903018\n",
      "Feature 306: 2.902969\n",
      "Feature 307: 2.909384\n",
      "Feature 308: 2.917775\n",
      "Feature 309: 2.931483\n",
      "Feature 310: 2.937423\n",
      "Feature 311: 2.945173\n",
      "Feature 312: 2.950513\n",
      "Feature 313: 2.960033\n",
      "Feature 314: 2.967760\n",
      "Feature 315: 2.980414\n",
      "Feature 316: 2.993064\n",
      "Feature 317: 3.000840\n",
      "Feature 318: 3.007072\n",
      "Feature 319: 3.012869\n",
      "Feature 320: 3.025739\n",
      "Feature 321: 3.034912\n",
      "Feature 322: 3.038425\n",
      "Feature 323: 3.042985\n",
      "Feature 324: 3.052407\n",
      "Feature 325: 3.060350\n",
      "Feature 326: 3.070538\n",
      "Feature 327: 3.078064\n",
      "Feature 328: 3.091471\n",
      "Feature 329: 3.107284\n",
      "Feature 330: 3.111437\n",
      "Feature 331: 0.173589\n",
      "Feature 332: 0.168144\n",
      "Feature 333: 0.169628\n",
      "Feature 334: 0.165091\n",
      "Feature 335: 0.168824\n",
      "Feature 336: 0.172387\n",
      "Feature 337: 0.168086\n",
      "Feature 338: 0.169766\n",
      "Feature 339: 0.168354\n",
      "Feature 340: 0.169537\n",
      "Feature 341: 0.172556\n",
      "Feature 342: 0.179034\n",
      "Feature 343: 0.175774\n",
      "Feature 344: 0.172620\n",
      "Feature 345: 0.180175\n",
      "Feature 346: 0.181202\n",
      "Feature 347: 0.185028\n",
      "Feature 348: 0.183745\n",
      "Feature 349: 0.182610\n",
      "Feature 350: 0.182182\n",
      "Feature 351: 0.178646\n",
      "Feature 352: 0.181995\n",
      "Feature 353: 0.183797\n",
      "Feature 354: 0.190736\n",
      "Feature 355: 0.182191\n",
      "Feature 356: 0.190190\n",
      "Feature 357: 0.190551\n",
      "Feature 358: 0.186012\n",
      "Feature 359: 0.182720\n",
      "Feature 360: 0.186937\n",
      "Feature 361: 0.003492\n",
      "Feature 362: 0.000000\n",
      "Feature 363: 0.000000\n",
      "Feature 364: 0.000000\n",
      "Feature 365: 0.000099\n",
      "Feature 366: 0.000630\n",
      "Feature 367: 0.002516\n",
      "Feature 368: 0.005204\n",
      "Feature 369: 0.000941\n",
      "Feature 370: 0.000000\n",
      "Feature 371: 0.000000\n",
      "Feature 372: 0.013725\n",
      "Feature 373: 0.001234\n",
      "Feature 374: 0.000000\n",
      "Feature 375: 0.000000\n",
      "Feature 376: 0.002505\n",
      "Feature 377: 0.001091\n",
      "Feature 378: 0.000000\n",
      "Feature 379: 0.000000\n",
      "Feature 380: 0.005330\n",
      "Feature 381: 0.000000\n",
      "Feature 382: 0.006811\n",
      "Feature 383: 0.001978\n",
      "Feature 384: 0.001243\n",
      "Feature 385: 0.000000\n",
      "Feature 386: 0.000000\n",
      "Feature 387: 0.002866\n",
      "Feature 388: 0.001678\n",
      "Feature 389: 0.000000\n",
      "Feature 390: 0.001660\n",
      "Feature 391: 0.003492\n",
      "Feature 392: 0.000000\n",
      "Feature 393: 0.000000\n",
      "Feature 394: 0.000000\n",
      "Feature 395: 0.000099\n",
      "Feature 396: 0.000630\n",
      "Feature 397: 0.002516\n",
      "Feature 398: 0.005204\n",
      "Feature 399: 0.000941\n",
      "Feature 400: 0.000000\n",
      "Feature 401: 0.000000\n",
      "Feature 402: 0.013725\n",
      "Feature 403: 0.001234\n",
      "Feature 404: 0.000000\n",
      "Feature 405: 0.000000\n",
      "Feature 406: 0.002505\n",
      "Feature 407: 0.001091\n",
      "Feature 408: 0.000000\n",
      "Feature 409: 0.000000\n",
      "Feature 410: 0.005330\n",
      "Feature 411: 0.000000\n",
      "Feature 412: 0.006811\n",
      "Feature 413: 0.001978\n",
      "Feature 414: 0.001243\n",
      "Feature 415: 0.000000\n",
      "Feature 416: 0.000000\n",
      "Feature 417: 0.002866\n",
      "Feature 418: 0.001678\n",
      "Feature 419: 0.000000\n",
      "Feature 420: 0.001660\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import SelectKBest, chi2,mutual_info_regression\n",
    "# configure to select all features\n",
    "fs = SelectKBest(score_func=mutual_info_regression, k='all')\n",
    "# learn relationship from training data\n",
    "fs.fit(X_train, Y_train)\n",
    "# transform train input data\n",
    "X_train_fs = fs.transform(X_train)\n",
    "print( X_train_fs, fs)\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# what are scores for the features\n",
    "for i in range(len(fs.scores_)):\n",
    "\tprint('Feature %d: %f' % (i, fs.scores_[i]))\n",
    "# plot the scores\n",
    "plt.bar([i for i in range(len(fs.scores_))], fs.scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "asset0 X_train[:,list(range(0, 31)) + list(range(91, 121)) + list(range(271, 361))]\n",
    "asset1 X_train[:,list(range(0, 31)) + list(range(91, 121)) + list(range(271, 361))]\n",
    "asset2 X_train[:,list(range(0, 61)) + list(range(91, 121)) + list(range(271, 331))]\n",
    "asset3 X_train[:,list(range(0, 31)) + list(range(91, 121)) + list(range(271, 361))]\n",
    "asset4 X_train[:,list(range(0, 31)) + list(range(91, 121)) + list(range(271, 361))]\n",
    "asset5 X_train[:,list(range(0, 31)) + list(range(91, 121)) + list(range(271, 361))]\n",
    "asset6 X_train[:,list(range(0, 31)) + list(range(91, 121)) + list(range(271, 361))]\n",
    "asset7 X_train[:,list(range(0, 1)) + list(range(91, 121)) + list(range(271, 361))]\n",
    "asset8 X_train[:,list(range(0, 1)) + list(range(271, 361))]\n",
    "asset9 X_train[:,list(range(0, 1)) + list(range(271, 361))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_input = X_train[:,list(range(0, 1)) + list(range(91, 121)) + list(range(271, 361))]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pickle\n",
    "\n",
    "reg = LinearRegression().fit(X_input, Y_train)\n",
    "pickle.dump(reg, open(f'LR{asset_num}.plk', 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}