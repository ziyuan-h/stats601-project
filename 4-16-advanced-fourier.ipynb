{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import random\n",
    "from string import ascii_letters\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.90 GiB for an array with shape (17709, 1440, 10) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Input \u001B[1;32mIn [4]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mojsim\u001B[39;00m\n\u001B[0;32m      2\u001B[0m sim \u001B[38;5;241m=\u001B[39m ojsim\u001B[38;5;241m.\u001B[39mOJSimulator()\n\u001B[1;32m----> 3\u001B[0m X,y \u001B[38;5;241m=\u001B[39m \u001B[43msim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformulized_train\u001B[49m\n\u001B[0;32m      4\u001B[0m X\u001B[38;5;241m.\u001B[39mshape, y\u001B[38;5;241m.\u001B[39mshape\n",
      "File \u001B[1;32mE:\\sjtu\\新建文件夹\\2022winter\\601\\proj\\stats601-project\\ojsim.py:39\u001B[0m, in \u001B[0;36mOJSimulator.formulized_train\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mformulized_train\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m---> 39\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset_\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_set_form\u001B[49m\n",
      "File \u001B[1;32mE:\\sjtu\\新建文件夹\\2022winter\\601\\proj\\stats601-project\\dataset.py:52\u001B[0m, in \u001B[0;36mDataSet.train_set_form\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_set_form\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformulized:\n\u001B[1;32m---> 52\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformulize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     53\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_f, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel_f\n",
      "File \u001B[1;32mE:\\sjtu\\新建文件夹\\2022winter\\601\\proj\\stats601-project\\dataset.py:34\u001B[0m, in \u001B[0;36mDataSet.formulize\u001B[1;34m(self, feature_size, step_size)\u001B[0m\n\u001B[0;32m     32\u001B[0m label_index \u001B[38;5;241m=\u001B[39m index[:,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m30\u001B[39m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m# print(index[:10])\u001B[39;00m\n\u001B[1;32m---> 34\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_f \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mstack([log_pr_train_np[index], \u001B[43mvolu_train_np\u001B[49m\u001B[43m[\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m]\u001B[49m], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel_f \u001B[38;5;241m=\u001B[39m log_pr_train_np[label_index]\u001B[38;5;241m.\u001B[39msqueeze()\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_f\u001B[38;5;241m.\u001B[39mshape, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel_f\u001B[38;5;241m.\u001B[39mshape\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 1.90 GiB for an array with shape (17709, 1440, 10) and data type float64"
     ]
    }
   ],
   "source": [
    "import ojsim\n",
    "sim = ojsim.OJSimulator()\n",
    "X,y = sim.formulized_train\n",
    "X.shape, y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# fourier on log price\n",
    "X_pr = X[-8000:,0,:,:]\n",
    "\n",
    "from joblib import delayed,Parallel, parallel_backend\n",
    "def inner(i,j):\n",
    "    window = 20\n",
    "    filtered = np.zeros(40,dtype=complex)\n",
    "    raw_fft = np.fft.fft(X_pr[i,:,j])\n",
    "    filtered[:window] = raw_fft[:window]\n",
    "    filtered[-window:] = raw_fft[-window:]\n",
    "    return np.fft.ifft(filtered)\n",
    "\n",
    "def outer(i):\n",
    "    return np.array(Parallel(n_jobs=-1)(delayed(inner)(i,j) for j in range(10)),dtype = complex)\n",
    "\n",
    "result = np.array(Parallel(n_jobs=-1)(delayed(outer)(i) for i in range(8000)),dtype = complex)\n",
    "input = np.concatenate((result.real, result.imag), axis = 2)\n",
    "input.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output = y[-8000:,:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def FastAllFeatureExtract(A):\n",
    "    d = 28\n",
    "    A0_pr = A[:, :1440]\n",
    "    A0_vo = A[:, 1440:] + 1\n",
    "    feature = A0_pr[:, -1] - A0_pr[:, 0]\n",
    "    # VO volumn log################\n",
    "    feature = np.concatenate((feature[:, None], np.log(A0_vo[:, -30+d:])), axis=1)\n",
    "\n",
    "    # VO moving avg (#sample,91:120)########\n",
    "    avg_step = 30\n",
    "    df_vo = pd.DataFrame(A0_vo[:, -59+d:].T)\n",
    "    ma_30 = lambda x: x.rolling(avg_step).mean()\n",
    "    df_vo.apply(ma_30).apply(np.log).T.to_numpy()[:, avg_step - 1:]\n",
    "    feature = np.append(feature, df_vo.apply(ma_30).apply(np.log).T.to_numpy()[:, -30+d:], axis=1)\n",
    "\n",
    "    # PR rate of change (#sample, 271:300)#######\n",
    "    df_pr = pd.DataFrame(A0_pr[:, -31:].T)\n",
    "    pct_chg_fxn = lambda x: x.pct_change()\n",
    "    # print(\"PR rate of change\", df_pr.apply(pct_chg_fxn).T.to_numpy()[:,-30:])\n",
    "    feature = np.append(feature, df_pr.apply(pct_chg_fxn).T.to_numpy()[:, -30+d:], axis=1)\n",
    "\n",
    "    # PR moving avg (#sample,301:330)  #######\n",
    "    df_pr = pd.DataFrame(A0_pr[:, -59:].T)\n",
    "    avg_step = 30\n",
    "    ma_30 = lambda x: x.rolling(avg_step).mean()\n",
    "    df_pr.apply(ma_30).T.to_numpy()[:, avg_step - 1:]\n",
    "    # print(\"PR moving avg\", df_pr.apply(ma_30).T.to_numpy()[:,-30:])\n",
    "    feature = np.append(feature, df_pr.apply(ma_30).T.to_numpy()[:, -30+d:], axis=1)\n",
    "\n",
    "    # PR binning (#sample, 331:360)#########\n",
    "    df_pr = pd.DataFrame(A0_pr[:, -30+d:].T)\n",
    "    n_bins = 10\n",
    "    bin_fxn = lambda y: pd.qcut(y, q=n_bins, labels=range(1, n_bins + 1))\n",
    "    binning = df_pr.apply(bin_fxn).T\n",
    "    # print(\"PR binning\", binning.to_numpy()[:,-30:])\n",
    "    feature = np.append(feature, binning.to_numpy(), axis=1)\n",
    "\n",
    "    return feature"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(8000, 10, 11)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract features\n",
    "X_vo = X[-8000:,1,:,:]\n",
    "X_total = np.concatenate((X_pr,X_vo),axis = 1)\n",
    "def Feature(i):\n",
    "    return FastAllFeatureExtract(X_total[:,:,i])\n",
    "feature_extract = np.array(Parallel(n_jobs=-1)(delayed(Feature)(i) for i in range(10)))\n",
    "feature_extract = feature_extract.swapaxes(0,1)\n",
    "feature_extract.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "((8000, 10, 91), (8000, 10))"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the input\n",
    "IInput =np.concatenate((input,feature_extract),axis=2)\n",
    "IInput.shape, output.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train the pairwise correlation\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from joblib import Parallel, delayed\n",
    "from pickle import dump\n",
    "def train(i):\n",
    "    reg = GradientBoostingRegressor().fit(IInput[:,i,:],output[:,i])\n",
    "    dump(reg, open(f'{i}.FFXGB','wb'))\n",
    "linear_models = Parallel(n_jobs=-1)(delayed(train)(i) for i in range(10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "linear_models = []\n",
    "for i in range(10):\n",
    "    linear_models.append(load(open(f'{i}.FFXGB','rb')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "OOutput = np.empty((8000,10),dtype=float)\n",
    "for i in range(8000):\n",
    "    OOutput[i,:] = np.array([linear_models[j].predict(IInput[i,[j],:]) for j in range(10)]).reshape(-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def get_batch(X, Y, batch_size=10000):\n",
    "    X, Y = X.float(), Y.float()\n",
    "    n = X.size()[0]\n",
    "    indices = torch.randint(low=0, high=n, size=(batch_size,))\n",
    "    return X[indices], Y[indices]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "IIIInput = OOutput - X_pr[:,-1,:]\n",
    "YYput = output - X_pr[:,-1,:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:13<3:45:44, 13.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(-0.5205, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1000 [00:33<4:51:23, 17.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor(-0.5287, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1000 [00:54<5:16:57, 19.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 tensor(-0.5090, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1000 [01:17<5:42:14, 20.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 tensor(-0.5502, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1000 [01:41<6:02:38, 21.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 tensor(-0.5137, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/1000 [02:06<6:17:00, 22.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 tensor(-0.5273, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/1000 [02:33<6:40:44, 24.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 tensor(-0.5409, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 8/1000 [02:59<6:51:38, 24.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 tensor(-0.5271, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9/1000 [03:28<7:11:10, 26.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 tensor(-0.4989, grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9/1000 [03:32<6:30:40, 23.65s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [12]\u001B[0m, in \u001B[0;36m<cell line: 37>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     42\u001B[0m corrs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack([torch\u001B[38;5;241m.\u001B[39mcorrcoef(stacked[i])[\u001B[38;5;241m0\u001B[39m,\u001B[38;5;241m1\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m10\u001B[39m)]) \u001B[38;5;66;03m# 10 x 1\u001B[39;00m\n\u001B[0;32m     43\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39mtorch\u001B[38;5;241m.\u001B[39mmean(corrs)\n\u001B[1;32m---> 44\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     45\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     46\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import  tqdm\n",
    "class Rescale(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Rescale, self).__init__()\n",
    "        # fc = []\n",
    "        # for i in range(10):\n",
    "        #     fc.append(\n",
    "        #         nn.Linear(1, 1)\n",
    "        #     )\n",
    "\n",
    "\n",
    "        # self.fc = nn.ModuleList(fc)\n",
    "        self.zzw = 10*torch.rand((10,1))#10*torch.FloatTensor(np.ones((10,1)))\n",
    "        self.zzw.requires_grad_()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print((x[:,0])[:,None].shape)\n",
    "        out = torch.empty((x.shape),dtype=torch.float)\n",
    "        for i in range(x.shape[0]):\n",
    "            for j in range(10):\n",
    "            #print(x[i,:].shape)\n",
    "                out[i,j] = x[i,j] * self.zzw[j] *self.zzw[j]\n",
    "                # out[i,j] = self.fc[j]((x[i,j])[None,None])\n",
    "        return out\n",
    "        #return torch.FloatTensor([self.fc[i]((x[:,i])[:,None]) for i in range(10)])\n",
    "\n",
    "\n",
    "\n",
    "final_model = Rescale()               # Moving the model to GPU if available\n",
    "criterion = nn.MSELoss()                         # One possible loss criterion\n",
    "learning_rate = 100\n",
    "optimizer = torch.optim.SGD([final_model.zzw],lr=learning_rate)#torch.optim.SGD\n",
    "# optimizer = torch.optim.SGD(final_model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "training_steps = 1000\n",
    "for i in tqdm(range(training_steps)):\n",
    "    minibatch_x, minibatch_y = get_batch(torch.FloatTensor(IIIInput), torch.FloatTensor(YYput))\n",
    "    output = final_model(minibatch_x)\n",
    "    stacked = torch.stack([output, minibatch_y]) #2 x b x 10\n",
    "    stacked = stacked.transpose(1,2).transpose(0,1) # 10 x 2 x b\n",
    "    corrs = torch.stack([torch.corrcoef(stacked[i])[0,1] for i in range(10)]) # 10 x 1\n",
    "    loss = -torch.mean(corrs)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(i,loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1.5849],\n        [6.2551],\n        [4.9892],\n        [1.2924],\n        [0.9898],\n        [9.3185],\n        [4.2212],\n        [2.9166],\n        [2.5077],\n        [4.1427]], requires_grad=True)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.zzw"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(final_model.state_dict(), \"final_scale01.mdl\")\n",
    "# load model\n",
    "# final_model = Rescale()\n",
    "# final_model.load_state_dict(torch.load(\"final_scale.mdl\"))\n",
    "# final_model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8496/8496 [04:19<00:00, 32.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time used: 259.117s\n",
      "Pairwise correlation:\n",
      "\tasset 0 = 0.00510\n",
      "\tasset 1 = 0.04444\n",
      "\tasset 2 = 0.04673\n",
      "\tasset 3 = 0.02996\n",
      "\tasset 4 = 0.05310\n",
      "\tasset 5 = 0.02349\n",
      "\tasset 6 = 0.03267\n",
      "\tasset 7 = 0.02625\n",
      "\tasset 8 = 0.04223\n",
      "\tasset 9 = 0.03679\n",
      "\tmean correlation = 0.03407\n",
      "Overall correlation: -0.01010\n",
      "===============================\n",
      "Fail to outperform Ziwei's method, whose pairwise average\n",
      "and overall correlations are (0.02840, 0.01536)\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_r_hat(A,B):\n",
    "    A = A.values.T\n",
    "    B = B.values.T\n",
    "    features = FastAllFeatureExtract(np.concatenate((A,B), axis = 1))\n",
    "    window = 20\n",
    "    def getNpredict(i):\n",
    "        filtered = np.zeros(40,dtype=complex)\n",
    "        raw_fft = np.fft.fft(A[i,:])\n",
    "        filtered[:window] = raw_fft[:window]\n",
    "        filtered[-window:] = raw_fft[-window:]\n",
    "        hi = np.append(filtered.real,filtered.imag)[None,:]\n",
    "        hi = np.concatenate((hi,features[[i],:]),axis = 1)\n",
    "        return linear_models[i].predict(hi)[0]- A[i,-1]\n",
    "    before_scale =  torch.FloatTensor([getNpredict(i) for i in range(10)] - A[:,-1])\n",
    "    after_scale =  final_model(before_scale[None,:])\n",
    "    #return np.array([getNpredict(i) for i in range(10)]) * np.array([3,8,4,8,4,1,8,4,10,9])#\n",
    "    return after_scale.detach().numpy()\n",
    "\n",
    "import ojsim\n",
    "sim = ojsim.OJSimulator()\n",
    "sim.submit(get_r_hat)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# without NN\n",
    "Total time used: 210.160s\n",
    "Pairwise correlation:\n",
    "\tasset 0 = 0.01060\n",
    "\tasset 1 = 0.04222\n",
    "\tasset 2 = 0.04612\n",
    "\tasset 3 = 0.02012\n",
    "\tasset 4 = 0.04990\n",
    "\tasset 5 = 0.02483\n",
    "\tasset 6 = 0.03063\n",
    "\tasset 7 = 0.02822\n",
    "\tasset 8 = 0.02789\n",
    "\tasset 9 = 0.04412\n",
    "\tmean correlation = 0.03247\n",
    "Overall correlation: -0.00005\n",
    "===============================\n",
    "Fail to outperform Ziwei's method, whose pairwise average\n",
    "and overall correlations are (0.02840, 0.01536)\n",
    "==============================="
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NN with bias\n",
    "Total time used: 214.427s\n",
    "Pairwise correlation:\n",
    "\tasset 0 = 0.00510\n",
    "\tasset 1 = 0.04444\n",
    "\tasset 2 = 0.04673\n",
    "\tasset 3 = 0.02996\n",
    "\tasset 4 = -0.05310\n",
    "\tasset 5 = -0.02349\n",
    "\tasset 6 = 0.03267\n",
    "\tasset 7 = -0.02625\n",
    "\tasset 8 = 0.04223\n",
    "\tasset 9 = -0.03679\n",
    "\tmean correlation = 0.00615\n",
    "Overall correlation: -0.01309\n",
    "===============================\n",
    "Fail to outperform Ziwei's method, whose pairwise average\n",
    "and overall correlations are (0.02840, 0.01536)\n",
    "==============================="
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\VApps\\Anaconda3\\envs\\stats601\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      " 20%|██        | 1736/8496 [00:47<03:06, 36.16it/s]"
     ]
    }
   ],
   "source": [
    "import ojsim\n",
    "import main\n",
    "sim = ojsim.OJSimulator()\n",
    "sim.submit(main.get_r_hat)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}