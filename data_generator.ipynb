{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "667dbefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "log_pr_file = './log_price.df'\n",
    "volu_usd_file = './volume_usd.df'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62fab46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pr = pd.read_pickle(log_pr_file)\n",
    "volu = pd.read_pickle(volu_usd_file)\n",
    "log_pr.columns = ['log_pr_%d'%i for i in range(10)]\n",
    "volu.columns = ['volu_%d'%i for i in range(10)]\n",
    "\n",
    "data = pd.concat([log_pr, volu], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbce5510",
   "metadata": {},
   "source": [
    "#### Dataset manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "905c57ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_data(data:pd.DataFrame, test_pct:float):\n",
    "    assert test_pct > 0 and test_pct < 1\n",
    "    test_size = int(len(data) * test_pct)\n",
    "    return  data[:-test_size], data[-test_size:]\n",
    "\n",
    "def split_data(log_pr:pd.DataFrame, volu:pd.DataFrame, test_pct:float):\n",
    "    return *_split_data(log_pr, test_pct), *_split_data(volu, test_pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41fb2495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formulize_data(data:pd.DataFrame, log_pr:pd.DataFrame, window_size=1440, step=10) -> np.array:\n",
    "    N = len(data)\n",
    "    assert N == len(log_pr)\n",
    "    train_index = np.arange(0, window_size)[np.newaxis, :] + np.arange(0, N - window_size - 30, step)[:, np.newaxis]\n",
    "    return_index = np.arange(0, N - window_size - 30, step)[:, np.newaxis] + window_size + 30 - 1\n",
    "#     print(train_index, return_index)\n",
    "    return data.values[train_index], log_pr.values[return_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ca8c99",
   "metadata": {},
   "source": [
    "#### Featue generator functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eed48be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_of_change(data:pd.DataFrame, periods):\n",
    "    return data.pct_change(periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91bfcf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(data:pd.DataFrame, window_size):\n",
    "    return data.rolling(window_size).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a384239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_moving_avg(data:pd.DataFrame, window_size):\n",
    "    return data.ewm(com = window_size - 1, adjust=True, min_periods = window_size).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26324a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(data:pd.DataFrame, window_size):\n",
    "    assert window_size > 1\n",
    "    return (data - data.rolling(window=window_size).mean()) / \\\n",
    "            data.rolling(window=window_size).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1587d174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_sum(data:pd.DataFrame, window_size):\n",
    "    return data.rolling(window_size).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6d2eea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign(data:pd.DataFrame):\n",
    "    return np.sign(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26ce30bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binning(data:pd.DataFrame, n_bins):\n",
    "    bin_fn = lambda y: pd.qcut(y, q=n_bins, labels=range(1, n_bins+1))\n",
    "    return data.apply(bin_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e05b649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RSI(data:pd.DataFrame, window_size, ema=True):\n",
    "    \"\"\"\n",
    "    Returns a pd.Series with the relative strength index.\n",
    "    \"\"\"\n",
    "    # Make two series: one for lower closes and one for higher closes\n",
    "    up = data.clip(lower=0)\n",
    "    down = -1 * data.clip(upper=0)\n",
    "    \n",
    "    if ema == True:\n",
    "        # Use exponential moving average\n",
    "        ma_up = up.ewm(com = window_size - 1, adjust=True, min_periods = window_size).mean()\n",
    "        ma_down = down.ewm(com = window_size - 1, adjust=True, min_periods = window_size).mean()\n",
    "    else:\n",
    "        # Use simple moving average\n",
    "        ma_up = up.rolling(window = window_size, adjust=False).mean()\n",
    "        ma_down = down.rolling(window = window_size, adjust=False).mean()\n",
    "        \n",
    "    rsi = ma_up / ma_down\n",
    "    rsi = 100 - (100/(1 + rsi))\n",
    "    return rsi.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "902c7384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MACD(data:pd.DataFrame, window_slow=26, window_fast=12, window_signal=9):\n",
    "    ema_slow = exp_moving_avg(data, window_slow)\n",
    "    ema_fast = exp_moving_avg(data, window_fast)\n",
    "    ema_signal = exp_moving_avg(data, window_signal)\n",
    "    macd = ema_fast - ema_slow\n",
    "    return macd, macd - ema_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1421e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_low(data:pd.DataFrame, window=52):\n",
    "    high = data.rolling(window=window).max()\n",
    "    low = data.rolling(window=window).min()\n",
    "    return high, low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f657363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RHP(data:pd.DataFrame, window=10, horizon=52):\n",
    "    high, low = high_low(data, horizon)\n",
    "    newhighs = (high == data).sum(axis=1)\n",
    "    newlows = (low == data).sum(axis=1)\n",
    "    rhp = newhighs / (newhighs + newlows + 1e-4)\n",
    "    return rhp.rolling(window=window).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd1a8379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BollingerBands(data:pd.DataFrame, window_size):\n",
    "    ma = data.rolling(window=window_size).mean()\n",
    "    sd = data.rolling(window=window_size).std()\n",
    "    up = ma + 2*sd\n",
    "    down = ma - 2*sd\n",
    "    return up, down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc84a8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TRIN(log_pr:pd.DataFrame, volu:pd.DataFrame, window=1):\n",
    "    num_stocks = log_pr.shape[1]\n",
    "    pr_adv = (log_pr.diff(window) > 0).sum(axis=1)\n",
    "    v_adv = (volu.diff(window) > 0).sum(axis=1)\n",
    "    trin = (pr_adv / (num_stocks - pr_adv + 1e-4)) / (v_adv / (num_stocks - v_adv + 1e-4) + 1e-4)\n",
    "    return trin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54beb37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADL(data:pd.DataFrame, window=1):\n",
    "    num = data.shape[1]\n",
    "    adv = (data.diff(window) > 0).sum(axis=1)\n",
    "    return adv - (num - adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a3631c",
   "metadata": {},
   "source": [
    "#### Feature generator pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34b2d1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct features\n",
    "def generate_features(log_pr:pd.DataFrame, volu:pd.DataFrame):\n",
    "    # 30 min negative returns\n",
    "    n_ret = -(log_pr - log_pr.shift(30))\n",
    "    n_ret.columns = ['30rt%d'%i for i in range(10)]\n",
    "    \n",
    "    # log volumes\n",
    "    log_volu = np.log(volu + 1)\n",
    "    log_volu.columns = ['lgv%d'%i for i in range(10)]\n",
    "    \n",
    "    # price diff\n",
    "    pr_diff = []\n",
    "    for d in [1, 10, 30]:\n",
    "        pr_diff.append(log_pr.diff(d))\n",
    "        pr_diff[-1].columns = ['prdif%d%d'%(d, i) for i in range(10)]\n",
    "        \n",
    "    # volume diff\n",
    "    volu_diff = []\n",
    "    for d in [1, 10, 30]:\n",
    "        volu_diff.append(volu.diff(d))\n",
    "        volu_diff[-1].columns = ['vdif%d%d'%(d, i) for i in range(10)]\n",
    "        \n",
    "    # price rate of change\n",
    "    pr_roc = []\n",
    "    for window in [1, 10, 30, 40, 60, 120, 1440 - 60]:\n",
    "        pr_roc.append(rate_of_change(log_pr, window))\n",
    "        pr_roc[-1].columns = ['prroc%d%d'%(window, i) for i in range(10)]\n",
    "        \n",
    "    # volume rate of change\n",
    "    v_roc = []\n",
    "    for window in [10, 30, 40, 60, 120, 1440 - 60]:\n",
    "        v_roc.append(rate_of_change(volu + 1, window))\n",
    "        v_roc[-1].columns = ['vroc%d%d'%(window, i) for i in range(10)]\n",
    "        \n",
    "    # price moving average\n",
    "    pr_ma =[]\n",
    "    for window in [1, 10, 30, 40, 60, 120, 1440 - 60]:\n",
    "        pr_ma.append(moving_average(log_pr, window))\n",
    "        pr_ma[-1].columns = ['prma%d%d'%(window, i) for i in range(10)]\n",
    "        \n",
    "    # volume moving average\n",
    "    v_ma = []\n",
    "    for window in [1, 10, 30, 40, 60, 120, 1440 - 60]:\n",
    "        v_ma.append(moving_average(volu, window))\n",
    "        v_ma[-1].columns = ['vma%d%d'%(window, i) for i in range(10)]\n",
    "        \n",
    "    # price exp moving average\n",
    "    pr_ema =[]\n",
    "    for window in [1, 10, 30, 40, 60, 120, 1440 - 60]:\n",
    "        pr_ema.append(exp_moving_avg(log_pr, window))\n",
    "        pr_ema[-1].columns = ['prema%d%d'%(window, i) for i in range(10)]\n",
    "        \n",
    "    # volume exp moving average\n",
    "    v_ema = []\n",
    "    for window in [1, 10, 30, 40, 60, 120, 1440 - 60]:\n",
    "        v_ema.append(exp_moving_avg(volu, window))\n",
    "        v_ema[-1].columns = ['vema%d%d'%(window, i) for i in range(10)]\n",
    "        \n",
    "    # z score\n",
    "    pr_z = []\n",
    "    for window in [10, 30, 40, 60, 120, 1440 - 60]:\n",
    "        pr_z.append(z_score(log_pr, window))\n",
    "        pr_z[-1].columns = ['prz%d%d'%(window, i) for i in range(10)]\n",
    "        \n",
    "    # volume z score\n",
    "    v_z = []\n",
    "    for window in [10, 30, 40, 60, 120, 1440 - 60]:\n",
    "        v_z.append(z_score(volu, window))\n",
    "        v_z[-1].columns = ['vz%d%d'%(window, i) for i in range(10)]\n",
    "        \n",
    "    # RSI indicators\n",
    "    rsi = RSI(log_pr, 14)\n",
    "    rsi.columns = ['rsi%d'%i for i in range(10)]\n",
    "    \n",
    "    # MACD indicators\n",
    "    macd, macd_r = MACD(log_pr)\n",
    "    macd.columns = ['macd%d'%i for i in range(10)]\n",
    "    macd_r.columns = ['macd%d'%i for i in range(10)]\n",
    "    \n",
    "    # high low indicators\n",
    "    high, low = high_low(log_pr)\n",
    "    high.columns = ['high%d'%i for i in range(10)]\n",
    "    low.columns = ['low%d'%i for i in range(10)]\n",
    "    \n",
    "    # high low ratio\n",
    "    rhp = RHP(log_pr)\n",
    "    rhp = pd.DataFrame(rhp, columns=['rhp'])\n",
    "    \n",
    "    # BollingerBands\n",
    "    upbb, downbb = BollingerBands(log_pr, 10)\n",
    "    upbb.columns = ['upbb%d'%i for i in range(10)]\n",
    "    downbb.columns = ['downbb%d'%i for i in range(10)]\n",
    "    \n",
    "    # TRIN\n",
    "    trin = TRIN(log_pr, volu)\n",
    "    trin = pd.DataFrame(trin, columns=['trin'])\n",
    "    \n",
    "    # ADL\n",
    "    adl = ADL(log_pr)\n",
    "    adl = pd.DataFrame(adl, columns=['adl'])\n",
    "    \n",
    "    import itertools\n",
    "    features = pd.concat([log_pr, volu, \n",
    "                          n_ret, log_volu, *pr_diff, *volu_diff, \n",
    "                          *pr_roc, *v_roc, *pr_ma, *v_ma, \n",
    "                          *pr_ema, *v_ema, *pr_z, *v_z,\n",
    "                          rsi, macd, high, low, rhp, \n",
    "                          upbb, downbb, trin, adl], axis=1)\n",
    "    return features\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75373b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = generate_features(log_pr, volu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38c4fb40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['log_pr_0', 'log_pr_1', 'log_pr_2', 'log_pr_3', 'log_pr_4', 'log_pr_5',\n",
       "       'log_pr_6', 'log_pr_7', 'log_pr_8', 'log_pr_9',\n",
       "       ...\n",
       "       'downbb2', 'downbb3', 'downbb4', 'downbb5', 'downbb6', 'downbb7',\n",
       "       'downbb8', 'downbb9', 'trin', 'adl'],\n",
       "      dtype='object', length=693)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e69a15cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.to_pickle('./features.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51f8b15",
   "metadata": {},
   "source": [
    "#### Forge training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a228de33",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pr_tr, log_pr_tst, volu_tr,  volu_tst = split_data(log_pr, volu, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd3c26b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((185472, 10), (79488, 10), (185472, 10), (79488, 10))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_pr_tr.shape, log_pr_tst.shape, volu_tr.shape, volu_tst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa4244c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_tr = generate_features(log_pr_tr, volu_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9122952c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((185472, 693),\n",
       " Index(['log_pr_0', 'log_pr_1', 'log_pr_2', 'log_pr_3', 'log_pr_4', 'log_pr_5',\n",
       "        'log_pr_6', 'log_pr_7', 'log_pr_8', 'log_pr_9',\n",
       "        ...\n",
       "        'downbb2', 'downbb3', 'downbb4', 'downbb5', 'downbb6', 'downbb7',\n",
       "        'downbb8', 'downbb9', 'trin', 'adl'],\n",
       "       dtype='object', length=693),\n",
       " (184092, 693))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_tr.shape, features.columns, features_tr.dropna().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a5fccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_tr.to_pickle('./features_tr.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17c0858b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18406, 3, 693), (18406, 1, 10))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_tr_f, label_tr_f = formulize_data(features_tr.dropna(), log_pr_tr.loc[features_tr.dropna().index], window_size=3)\n",
    "features_tr_f.shape, label_tr_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe14ad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./features_tr_f.pkl', features_tr_f)\n",
    "np.save('./labels_tr_f.pkl', label_tr_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da7fa0b",
   "metadata": {},
   "source": [
    "#### Train and save a simple linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9d90d7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9988272726243963"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "N = len(features_tr_f)\n",
    "model = LinearRegression()\n",
    "model.fit(features_tr_f.reshape(N, -1), label_tr_f.squeeze())\n",
    "model.score(features_tr_f.reshape(N, -1), label_tr_f.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3ff81e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the file\n",
    "import pickle\n",
    "with open('./feature_linreg.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e8b34912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./feature_linreg.pkl', 'rb') as f:\n",
    "    file_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "811bdf62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7805/7805 [09:57<00:00, 13.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time used: 597.955s\n",
      "Pairwise correlation:\n",
      "\tasset 0 = 0.00922\n",
      "\tasset 1 = -0.00610\n",
      "\tasset 2 = 0.01215\n",
      "\tasset 3 = 0.01271\n",
      "\tasset 4 = 0.05252\n",
      "\tasset 5 = -0.02366\n",
      "\tasset 6 = 0.04410\n",
      "\tasset 7 = 0.00385\n",
      "\tasset 8 = -0.05468\n",
      "\tasset 9 = -0.00688\n",
      "\tmean correlation = 0.00432\n",
      "Overall correlation: 0.00634\n",
      "===============================\n",
      "Fail to outperform Ziwei's method, whose pairwise average\n",
      "and overall correlations are (0.02840, 0.01536)\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(597.9553079605103,\n",
       " 0    0.009221\n",
       " 1   -0.006103\n",
       " 2    0.012149\n",
       " 3    0.012707\n",
       " 4    0.052524\n",
       " 5   -0.023663\n",
       " 6    0.044095\n",
       " 7    0.003845\n",
       " 8   -0.054679\n",
       " 9   -0.006883\n",
       " dtype: float64,\n",
       " 0.0063425222829207656)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from critic import Critic\n",
    "cr = Critic()\n",
    "from tqdm import tqdm\n",
    "def get_r_hat(A, B):\n",
    "#     print(A.shape, B.shape)\n",
    "    features = generate_features(A, B).dropna()\n",
    "    features_tr = features[-3:]\n",
    "    # print(features_tr.shape)\n",
    "    pred = file_model.predict(features_tr.values.reshape(1, -1))\n",
    "    return pred - A.values[-1]\n",
    "    \n",
    "cr.submit(get_r_hat, log_pr_tst, volu_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ada7198",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d508d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x720 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from joblib import delayed, Parallel\n",
    "from sklearn.feature_selection import f_regression, SelectKBest\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = len(features_tr_f)\n",
    "\n",
    "def plot_corr_score(features, label):\n",
    "    fs = SelectKBest(score_func=f_regression, k='all')\n",
    "    fs.fit(features, label)\n",
    "    return np.concatenate([fs.scores_, fs.pvalues_], axis=0)\n",
    "\n",
    "rank_array_cor = Parallel(n_jobs=6)(delayed(plot_corr_score)(features_tr_f.reshape(N, -1), label_tr_f[:,:,i]) for i in range(10))\n",
    "\n",
    "_, ax = plt.subplots(5, 2, figsize=(12,10))\n",
    "for i, rank in enumerate(rank_array_cor):\n",
    "    p_values = rank[len(rank)//2:]\n",
    "    ax[i // 2, i % 2].bar([i for i in range(len(p_values))], p_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0942023a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x720 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, ax = plt.subplots(5, 2, figsize=(12,10))\n",
    "for i, rank in enumerate(rank_array_cor):\n",
    "    f_values = rank[:len(rank)//2]\n",
    "    ax[i // 2, i % 2].bar([i for i in range(len(f_values))], f_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features of asset 0 with p-value < 0.0010: num = 1193\n",
      "features of asset 1 with p-value < 0.0010: num = 1184\n",
      "features of asset 2 with p-value < 0.0010: num = 1134\n",
      "features of asset 3 with p-value < 0.0010: num = 1181\n",
      "features of asset 4 with p-value < 0.0010: num = 1181\n",
      "features of asset 5 with p-value < 0.0010: num = 974\n",
      "features of asset 6 with p-value < 0.0010: num = 1145\n",
      "features of asset 7 with p-value < 0.0010: num = 1117\n",
      "features of asset 8 with p-value < 0.0010: num = 1078\n",
      "features of asset 9 with p-value < 0.0010: num = 1072\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.001\n",
    "selected_cor_p = []\n",
    "for i, rank in enumerate(rank_array_cor):\n",
    "    # print(rank)\n",
    "    p_values = rank[len(rank)//2:]\n",
    "    selected_cor_p.append(np.where(p_values < threshold)[0])\n",
    "    print(\"features of asset %d with p-value < %.4f: num = %d\"%(i, threshold, len(selected_cor_p[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33481a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features of asset 0 with f-value < 6366.0000: num = 381\n",
      "features of asset 1 with f-value < 6366.0000: num = 420\n",
      "features of asset 2 with f-value < 6366.0000: num = 444\n",
      "features of asset 3 with f-value < 6366.0000: num = 348\n",
      "features of asset 4 with f-value < 6366.0000: num = 441\n",
      "features of asset 5 with f-value < 6366.0000: num = 192\n",
      "features of asset 6 with f-value < 6366.0000: num = 150\n",
      "features of asset 7 with f-value < 6366.0000: num = 417\n",
      "features of asset 8 with f-value < 6366.0000: num = 183\n",
      "features of asset 9 with f-value < 6366.0000: num = 507\n"
     ]
    }
   ],
   "source": [
    "threshold = 6366\n",
    "selected_cor_f = []\n",
    "for i, rank in enumerate(rank_array_cor):\n",
    "    # print(rank)\n",
    "    f_values = rank[:len(rank)//2]\n",
    "    selected_cor_f.append(np.where(f_values > threshold)[0])\n",
    "    print(\"features of asset %d with f-value < %.4f: num = %d\"%(i, threshold, len(selected_cor_f[-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aec537",
   "metadata": {},
   "source": [
    "Find all features with significant p-value and significant f-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0fbffb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features selected with correlation score for asset 0: 381\n",
      "features selected with correlation score for asset 1: 420\n",
      "features selected with correlation score for asset 2: 444\n",
      "features selected with correlation score for asset 3: 348\n",
      "features selected with correlation score for asset 4: 441\n",
      "features selected with correlation score for asset 5: 192\n",
      "features selected with correlation score for asset 6: 150\n",
      "features selected with correlation score for asset 7: 417\n",
      "features selected with correlation score for asset 8: 183\n",
      "features selected with correlation score for asset 9: 507\n"
     ]
    }
   ],
   "source": [
    "selected_cor = []\n",
    "for i in range(10):\n",
    "    selected_cor.append(np.intersect1d(selected_cor_p[i], selected_cor_f[i], True))\n",
    "    print(\"features selected with correlation score for asset %d: %d\"%(i, len(selected_cor[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c513a3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mutual information\n",
    "from joblib import delayed, Parallel\n",
    "from sklearn.feature_selection import mutual_info_regression, SelectKBest\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_corr_score(features, label, ax):\n",
    "    fs = SelectKBest(score_func=mutual_info_regression, k='all')\n",
    "    fs.fit(features, label)\n",
    "    return fs.scores_\n",
    "\n",
    "score_array = Parallel(n_jobs=6)(delayed(plot_corr_score)(features_tr_f.reshape(N,-1), label_tr_f[:,:,i], ax) for i in range(10))\n",
    "\n",
    "_, ax = plt.subplots(5, 2, figsize=(10,8))\n",
    "for i, scores in enumerate(score_array):\n",
    "    ax[i // 2, i % 2].bar([i for i in range(len(scores))], scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d0761623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features of asset 0 with mi-value > 1.0000: num = 624\n",
      "features of asset 1 with mi-value > 1.0000: num = 636\n",
      "features of asset 2 with mi-value > 1.0000: num = 633\n",
      "features of asset 3 with mi-value > 1.0000: num = 621\n",
      "features of asset 4 with mi-value > 1.0000: num = 632\n",
      "features of asset 5 with mi-value > 1.0000: num = 630\n",
      "features of asset 6 with mi-value > 1.0000: num = 615\n",
      "features of asset 7 with mi-value > 1.0000: num = 621\n",
      "features of asset 8 with mi-value > 1.0000: num = 605\n",
      "features of asset 9 with mi-value > 1.0000: num = 639\n"
     ]
    }
   ],
   "source": [
    "threshold = 1\n",
    "selected_mi = []\n",
    "for i, score in enumerate(score_array):\n",
    "    selected_mi.append(np.where(score > threshold)[0])\n",
    "    print(\"features of asset %d with mi-value > %.4f: num = %d\"%(i, threshold, len(selected_mi[-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7508da",
   "metadata": {},
   "source": [
    "Unique features that might have non-linear relationship with target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5123894e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features of asset 0 that assumed to be linear but not significant: num=30\n",
      "features of asset 1 that assumed to be linear but not significant: num=9\n",
      "features of asset 2 that assumed to be linear but not significant: num=30\n",
      "features of asset 3 that assumed to be linear but not significant: num=6\n",
      "features of asset 4 that assumed to be linear but not significant: num=31\n",
      "features of asset 5 that assumed to be linear but not significant: num=12\n",
      "features of asset 6 that assumed to be linear but not significant: num=24\n",
      "features of asset 7 that assumed to be linear but not significant: num=9\n",
      "features of asset 8 that assumed to be linear but not significant: num=6\n",
      "features of asset 9 that assumed to be linear but not significant: num=30\n"
     ]
    }
   ],
   "source": [
    "non_sig_linear_features = []\n",
    "for i in range(10):\n",
    "    non_sig_linear_features.append(np.setdiff1d(selected_cor[i], selected_mi[i], True))\n",
    "    print(\"features of asset %d that assumed to be linear but not significant: num=%d\"%(i, len(non_sig_linear_features[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "65ad2ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features of asset 0 that assumed to be linear: num=351\n",
      "features of asset 1 that assumed to be linear: num=411\n",
      "features of asset 2 that assumed to be linear: num=414\n",
      "features of asset 3 that assumed to be linear: num=342\n",
      "features of asset 4 that assumed to be linear: num=410\n",
      "features of asset 5 that assumed to be linear: num=180\n",
      "features of asset 6 that assumed to be linear: num=126\n",
      "features of asset 7 that assumed to be linear: num=408\n",
      "features of asset 8 that assumed to be linear: num=177\n",
      "features of asset 9 that assumed to be linear: num=477\n"
     ]
    }
   ],
   "source": [
    "linear_features = []\n",
    "for i in range(10):\n",
    "    linear_features.append(np.intersect1d(selected_cor[i], selected_mi[i], True))\n",
    "    print(\"features of asset %d that assumed to be linear: num=%d\"%(i, len(linear_features[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "295f8997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features of asset 0 that assumed to be non-linear: num=273\n",
      "features of asset 1 that assumed to be non-linear: num=225\n",
      "features of asset 2 that assumed to be non-linear: num=219\n",
      "features of asset 3 that assumed to be non-linear: num=279\n",
      "features of asset 4 that assumed to be non-linear: num=222\n",
      "features of asset 5 that assumed to be non-linear: num=450\n",
      "features of asset 6 that assumed to be non-linear: num=489\n",
      "features of asset 7 that assumed to be non-linear: num=213\n",
      "features of asset 8 that assumed to be non-linear: num=428\n",
      "features of asset 9 that assumed to be non-linear: num=162\n"
     ]
    }
   ],
   "source": [
    "nonlinear_features = []\n",
    "for i in range(10):\n",
    "    nonlinear_features.append(np.setdiff1d(selected_mi[i], selected_cor[i], True))\n",
    "    print(\"features of asset %d that assumed to be non-linear: num=%d\"%(i, len(nonlinear_features[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "569a47d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./selected_features_idx.pkl', 'wb') as f:\n",
    "    selected_idx = {'nonlin': nonlinear_features, 'lin': linear_features, 'nonsig': non_sig_linear_features}\n",
    "    pickle.dump(selected_idx, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36abb3b",
   "metadata": {},
   "source": [
    "#### Classify all those features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c1845e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201,)\n",
      "['downbb0' 'downbb1' 'downbb2' 'downbb3' 'downbb4' 'downbb5' 'downbb6'\n",
      " 'downbb7' 'downbb8' 'downbb9' 'high0' 'high1' 'high2' 'high3' 'high4'\n",
      " 'high5' 'high6' 'high7' 'high8' 'high9'] ['upbb1' 'upbb2' 'upbb3' 'upbb4' 'upbb5' 'upbb6' 'upbb7' 'upbb8' 'upbb9'\n",
      " 'vema1202' 'vema13802' 'vema13803' 'vema13805' 'vema13806' 'vema402'\n",
      " 'vema602' 'vma1202' 'vma13802' 'vma13805' 'vma13806']\n"
     ]
    }
   ],
   "source": [
    "# train using linear models only\n",
    "linear_features_name = np.array([],dtype=str)\n",
    "for i in range(10):\n",
    "    for _ in range(features_tr_f.shape[1] - 1):\n",
    "        linear_features[i][np.where(linear_features[i] >= 693)[0]] -= 693\n",
    "    linear_features_name = np.union1d(linear_features_name, features_tr.columns[linear_features[i]])\n",
    "print(linear_features_name.shape)\n",
    "print(linear_features_name[:20], linear_features_name[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "d4fa6b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(212,)\n",
      "['downbb0' 'downbb1' 'downbb2' 'downbb3' 'downbb4' 'downbb5' 'downbb6'\n",
      " 'downbb7' 'downbb8' 'downbb9' 'high0' 'high1' 'high2' 'high3' 'high4'\n",
      " 'high5' 'high6' 'high7' 'high8' 'high9'] ['vema13801' 'vema13802' 'vema13803' 'vema13804' 'vema13805' 'vema13806'\n",
      " 'vema13807' 'vema13808' 'vema13809' 'vma1202' 'vma13800' 'vma13801'\n",
      " 'vma13802' 'vma13803' 'vma13804' 'vma13805' 'vma13806' 'vma13807'\n",
      " 'vma13808' 'vma13809']\n"
     ]
    }
   ],
   "source": [
    "# train using non linear models only\n",
    "nonlinear_features_name = np.array([],dtype=str)\n",
    "for i in range(10):\n",
    "    for _ in range(features_tr_f.shape[1] - 1):\n",
    "        nonlinear_features[i][np.where(nonlinear_features[i] >= 693)[0]] -= 693\n",
    "    nonlinear_features_name = np.union1d(nonlinear_features_name, features_tr.columns[nonlinear_features[i]])\n",
    "print(nonlinear_features_name.shape)\n",
    "print(nonlinear_features_name[:20], nonlinear_features_name[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "f00d660d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26,)\n",
      "['lgv2' 'rsi3' 'rsi5' 'rsi6' 'rsi7' 'rsi8' 'rsi9' 'vema102' 'vema1202'\n",
      " 'vema1205' 'vema1206' 'vema302' 'vema306' 'vema402' 'vema406' 'vema602'\n",
      " 'vema605' 'vema606' 'vma102' 'vma1202'] ['rsi9' 'vema102' 'vema1202' 'vema1205' 'vema1206' 'vema302' 'vema306'\n",
      " 'vema402' 'vema406' 'vema602' 'vema605' 'vema606' 'vma102' 'vma1202'\n",
      " 'vma1205' 'vma1206' 'vma302' 'vma402' 'vma602' 'vma606']\n"
     ]
    }
   ],
   "source": [
    "# train using non-significant linear models only\n",
    "nonsig_linear_features_name = np.array([],dtype=str)\n",
    "for i in range(10):\n",
    "    for _ in range(features_tr_f.shape[1] - 1):\n",
    "        non_sig_linear_features[i][np.where(non_sig_linear_features[i] >= 693)[0]] -= 693\n",
    "    nonsig_linear_features_name = np.union1d(nonsig_linear_features_name, features_tr.columns[non_sig_linear_features[i]])\n",
    "print(nonsig_linear_features_name.shape)\n",
    "print(nonsig_linear_features_name[:20], nonsig_linear_features_name[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "cf95d21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236\n",
      "(33,)\n",
      "['downbb' 'high' 'lgv' 'log_pr_' 'low' 'prema1' 'prema10' 'prema120'\n",
      " 'prema1380' 'prema30' 'prema40' 'prema60' 'prma1' 'prma10' 'prma120'\n",
      " 'prma1380' 'prma30' 'prma40' 'prma60' 'rsi' 'upbb' 'vema10' 'vema120'\n",
      " 'vema1380' 'vema30' 'vema40' 'vema60' 'vma10' 'vma120' 'vma1380' 'vma30'\n",
      " 'vma40' 'vma60']\n"
     ]
    }
   ],
   "source": [
    "# total features need to be constructed\n",
    "feature_names = np.union1d(linear_features_name, nonlinear_features_name)\n",
    "feature_names = np.union1d(feature_names, nonsig_linear_features_name)\n",
    "print(len(feature_names))\n",
    "feature_names = np.unique([name[:-1] for name in feature_names])\n",
    "feature_names.sort()\n",
    "print(feature_names.shape)\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "78b4b815",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./selecetd_featuers_names.pkl', 'wb') as f:\n",
    "    selected_features = {\"nonlin\": nonlinear_features_name, \"lin\": linear_features_name, \"nonsig\": nonsig_linear_features_name}\n",
    "    pickle.dump(selected_features, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f865ce3",
   "metadata": {},
   "source": [
    "#### Rewrite feature extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "6494bab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(log_pr:pd.DataFrame, volu:pd.DataFrame):\n",
    "    # Bollinger Bands\n",
    "    upbb, downbb = BollingerBands(log_pr, 10)\n",
    "    upbb.columns = ['upbb%d'%i for i in range(10)]\n",
    "    downbb.columns = ['downbb%d'%i for i in range(10)]\n",
    "\n",
    "    # high low indicators\n",
    "    high, low = high_low(log_pr)\n",
    "    high.columns = ['high%d'%i for i in range(10)]\n",
    "    low.columns = ['low%d'%i for i in range(10)]\n",
    "\n",
    "    # price exp moving average\n",
    "    pr_ema =[]\n",
    "    for window in [1, 10, 30, 40, 60, 120, 1440 - 60]:\n",
    "        pr_ema.append(exp_moving_avg(log_pr, window))\n",
    "        pr_ema[-1].columns = ['prema%d%d'%(window, i) for i in range(10)]\n",
    "\n",
    "    # price moving average\n",
    "    pr_ma =[]\n",
    "    for window in [1, 10, 30, 40, 60, 120, 1440 - 60]:\n",
    "        pr_ma.append(moving_average(log_pr, window))\n",
    "        pr_ma[-1].columns = ['prma%d%d'%(window, i) for i in range(10)]\n",
    "\n",
    "    # volume exp moving average\n",
    "    v_ema = []\n",
    "    for window in [10, 30, 40, 60, 120, 1440 - 60]:\n",
    "        v_ema.append(exp_moving_avg(volu, window))\n",
    "        v_ema[-1].columns = ['vema%d%d'%(window, i) for i in range(10)]\n",
    "    \n",
    "    # volume moving average\n",
    "    v_ma = []\n",
    "    for window in [10, 30, 40, 60, 120, 1440 - 60]:\n",
    "        v_ma.append(moving_average(volu, window))\n",
    "        v_ma[-1].columns = ['vma%d%d'%(window, i) for i in range(10)]\n",
    "\n",
    "    # log volumes\n",
    "    log_volu = np.log(volu['volu_2'] + 1)\n",
    "    log_volu = pd.DataFrame(log_volu)\n",
    "    log_volu.columns = ['lgv2']\n",
    "\n",
    "    # rsi\n",
    "    rsi = RSI(log_pr[['log_pr_%d'%i for i in [3,5,6,7,8,9]]], 14)\n",
    "    rsi.columns = ['rsi%d'%i for i in [3,5,6,7,8,9]]\n",
    "   \n",
    "    features = pd.concat([*pr_ma, *v_ma, \n",
    "                          *pr_ema, *v_ema, high, low,\n",
    "                          upbb, downbb, log_volu, rsi], axis=1)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a8ede86e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(185472, 307)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_tr_new = get_features(log_pr_tr, volu_tr)\n",
    "features_tr_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "3eeeddb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307,)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_tr_new.columns.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f159f6",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "f3e66946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191,)\n",
      "(18406, 3, 191)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9955548921487966"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "for name in ['log_pr_%d'%i for i in range(10)]:\n",
    "    # features_tr_new = features_tr_new.drop(name, axis=1)\n",
    "    linear_features_name = np.delete(linear_features_name, np.where(linear_features_name==name)[0])\n",
    "print(linear_features_name.shape)\n",
    "# for name in ['volu_%d'%i for i in range(10)]:\n",
    "    # features_tr_new = features_tr_new.drop(name, axis=1)\n",
    "    # linear_features_name = np.delete(linear_features_name, np.where(linear_features_name==name)[0])\n",
    "\n",
    "X, y = formulize_data(features_tr_new[linear_features_name].dropna(), log_pr_tr.loc[features_tr_new[linear_features_name].dropna().index], window_size=3)\n",
    "print(X.shape)\n",
    "N = len(X)\n",
    "reg = LinearRegression().fit(X.reshape(N, -1)[N//2:], y.squeeze()[N//2:])\n",
    "reg.score(X.reshape(N, -1)[N//2:], y.squeeze()[N//2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "3959c113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7805/7805 [04:13<00:00, 30.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time used: 253.598s\n",
      "Pairwise correlation:\n",
      "\tasset 0 = -0.02643\n",
      "\tasset 1 = 0.00091\n",
      "\tasset 2 = -0.02805\n",
      "\tasset 3 = -0.00144\n",
      "\tasset 4 = 0.03917\n",
      "\tasset 5 = -0.02209\n",
      "\tasset 6 = -0.02543\n",
      "\tasset 7 = -0.02036\n",
      "\tasset 8 = 0.03133\n",
      "\tasset 9 = 0.02825\n",
      "\tmean correlation = -0.00241\n",
      "Overall correlation: 0.00039\n",
      "===============================\n",
      "Fail to outperform Ziwei's method, whose pairwise average\n",
      "and overall correlations are (0.02840, 0.01536)\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(253.59802889823914,\n",
       " 0   -0.026430\n",
       " 1    0.000910\n",
       " 2   -0.028048\n",
       " 3   -0.001436\n",
       " 4    0.039167\n",
       " 5   -0.022092\n",
       " 6   -0.025429\n",
       " 7   -0.020360\n",
       " 8    0.031328\n",
       " 9    0.028246\n",
       " dtype: float64,\n",
       " 0.0003933611327430941)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_r_hat(A, B):\n",
    "    f = get_features(A, B)[linear_features_name]\n",
    "    # print(f.shape)\n",
    "    X, _ = formulize_data(f.dropna(), A.loc[f.dropna().index], window_size=3)\n",
    "    # print(X.shape)\n",
    "    pred = reg.predict(X[-1].reshape(1,-1))\n",
    "    return pred - A.values[-1]\n",
    "\n",
    "import critic \n",
    "cr = critic.Critic()\n",
    "cr.submit(get_r_hat, log_pr_tst, volu_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7875dd09",
   "metadata": {},
   "source": [
    "Total time used: 258.373s\n",
    "Pairwise correlation:\n",
    "\tasset 0 = -0.00861\n",
    "\tasset 1 = 0.01755\n",
    "\tasset 2 = 0.03186\n",
    "\tasset 3 = 0.01505\n",
    "\tasset 4 = -0.02840\n",
    "\tasset 5 = -0.01365\n",
    "\tasset 6 = 0.00502\n",
    "\tasset 7 = -0.01361\n",
    "\tasset 8 = 0.06049\n",
    "\tasset 9 = 0.04462\n",
    "\tmean correlation = 0.01103\n",
    "Overall correlation: 0.01162\n",
    "===============================\n",
    "Fail to outperform Ziwei's method, whose pairwise average\n",
    "and overall correlations are (0.02840, 0.01536)\n",
    "==============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5cc42237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7805/7805 [04:20<00:00, 30.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time used: 260.099s\n",
      "Pairwise correlation:\n",
      "\tasset 0 = -0.03011\n",
      "\tasset 1 = 0.01958\n",
      "\tasset 2 = -0.04135\n",
      "\tasset 3 = -0.00115\n",
      "\tasset 4 = -0.05459\n",
      "\tasset 5 = -0.00020\n",
      "\tasset 6 = 0.03621\n",
      "\tasset 7 = 0.02085\n",
      "\tasset 8 = 0.05259\n",
      "\tasset 9 = 0.04693\n",
      "\tmean correlation = 0.00487\n",
      "Overall correlation: -0.00318\n",
      "===============================\n",
      "Fail to outperform Ziwei's method, whose pairwise average\n",
      "and overall correlations are (0.02840, 0.01536)\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(260.0986053943634,\n",
       " 0   -0.030105\n",
       " 1    0.019581\n",
       " 2   -0.041351\n",
       " 3   -0.001152\n",
       " 4   -0.054594\n",
       " 5   -0.000202\n",
       " 6    0.036207\n",
       " 7    0.020847\n",
       " 8    0.052585\n",
       " 9    0.046926\n",
       " dtype: float64,\n",
       " -0.0031800294816325386)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_r_hat(A, B):\n",
    "    f = get_features(A, B)\n",
    "    # print(f.shape)\n",
    "    X, _ = formulize_data(f.dropna(), A.loc[f.dropna().index], window_size=3)\n",
    "    # print(X.shape)\n",
    "    pred = reg.predict(X.reshape(len(X),-1)).mean()\n",
    "    return pred - A.values[-1]\n",
    "\n",
    "import critic \n",
    "cr = critic.Critic()\n",
    "cr.submit(get_r_hat, log_pr_tst, volu_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7b61ee64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 1: 0.8540; 0, 2: 0.8345; 0, 3: 0.4903; 0, 4: 0.8406; 0, 5: 0.1274; 0, 6: 0.0000; 0, 7: 0.8467; 0, 8: 0.1355; 0, 9: 0.7250; \n",
      "1, 2: 0.9784; 1, 3: 0.6090; 1, 4: 0.9855; 1, 5: 0.1130; 1, 6: 0.1187; 1, 7: 0.9927; 1, 8: 0.1200; 1, 9: 0.6536; \n",
      "2, 3: 0.6051; 2, 4: 0.9928; 2, 5: 0.1124; 2, 6: 0.1180; 2, 7: 0.9855; 2, 8: 0.1193; 2, 9: 0.6685; \n",
      "3, 4: 0.6090; 3, 5: 0.0000; 3, 6: 0.3220; 3, 7: 0.6129; 3, 8: 0.0000; 3, 9: 0.3858; \n",
      "4, 5: 0.1130; 4, 6: 0.1187; 4, 7: 0.9927; 4, 8: 0.1200; 4, 9: 0.6629; \n",
      "5, 6: 0.0200; 5, 7: 0.1136; 5, 8: 0.9508; 5, 9: 0.3688; \n",
      "6, 7: 0.1195; 6, 8: 0.0000; 6, 9: 0.0050; \n",
      "7, 8: 0.1207; 7, 9: 0.6573; \n",
      "8, 9: 0.3711; \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    for j in range(i+1, 10):\n",
    "        a = np.intersect1d(linear_features[i], linear_features[j])\n",
    "        b = np.union1d(linear_features[i], linear_features[j])\n",
    "        print('%d, %d: %.4f'%(i, j, len(a)/len(b)), end='; ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a134325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(1,2,4,7), 0, 3, 5, 6, 8, 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "687a8a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = {}\n",
    "for i in [0,3,5,6,8,9]:\n",
    "    X['%d'%i] = features_tr_new[features_tr.columns[linear_features[i]]]\n",
    "X['1247'] = features_tr_new[\n",
    "                np.union1d([],\n",
    "                    np.concatenate([features_tr.columns[linear_features[1]],\n",
    "                        features_tr.columns[linear_features[2]],\n",
    "                        features_tr.columns[linear_features[4]],\n",
    "                        features_tr.columns[linear_features[7]]])\n",
    "                )\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ec4c5a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9988642356919979\n",
      "0.998711703970563\n",
      "0.9989116266461106\n",
      "0.9969895317332763\n",
      "0.9969737702539483\n",
      "0.9993887021365492\n",
      "0.9992633974788883\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "for i in [0,3,5,6,8,9]:\n",
    "    Xtr, ytr = formulize_data(X['%d'%i].dropna(), log_pr_tr.loc[X['%d'%i].dropna().index], 3)\n",
    "    models['%d'%i] = LinearRegression().fit(Xtr.reshape(N,-1), ytr[:,:,i])\n",
    "    print(models['%d'%i].score(Xtr.reshape(N,-1), ytr[:,:,i]))\n",
    "\n",
    "Xtr, ytr = formulize_data(X['1247'].dropna(), log_pr_tr.loc[X['1247'].dropna().index], 3)\n",
    "models['1247'] = LinearRegression().fit(Xtr.reshape(N,-1), ytr[:,:, [1,2,4,7]].squeeze())\n",
    "print(models['1247'].score(Xtr.reshape(N,-1), ytr[:,:, [1,2,4,7]].squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "809a5d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7805/7805 [04:47<00:00, 27.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time used: 287.924s\n",
      "Pairwise correlation:\n",
      "\tasset 0 = 0.01187\n",
      "\tasset 1 = 0.02832\n",
      "\tasset 2 = -0.01719\n",
      "\tasset 3 = 0.02702\n",
      "\tasset 4 = -0.03086\n",
      "\tasset 5 = -0.00439\n",
      "\tasset 6 = -0.00226\n",
      "\tasset 7 = 0.03311\n",
      "\tasset 8 = 0.08610\n",
      "\tasset 9 = -0.02633\n",
      "\tmean correlation = 0.01054\n",
      "Overall correlation: 0.00190\n",
      "===============================\n",
      "Fail to outperform Ziwei's method, whose pairwise average\n",
      "and overall correlations are (0.02840, 0.01536)\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(287.923663854599,\n",
       " 0    0.011871\n",
       " 1    0.028321\n",
       " 2   -0.017190\n",
       " 3    0.027022\n",
       " 4   -0.030864\n",
       " 5   -0.004388\n",
       " 6   -0.002258\n",
       " 7    0.033114\n",
       " 8    0.086096\n",
       " 9   -0.026333\n",
       " dtype: float64,\n",
       " 0.0019030997814779857)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_r_hat(A, B):\n",
    "    features_tr_new = get_features(A, B)\n",
    "    # print(f.shape)\n",
    "    pred = np.zeros(10)\n",
    "    X = {}\n",
    "    for i in [0,3,5,6,8,9]:\n",
    "        Xtr = features_tr_new[features_tr.columns[linear_features[i]]].values[-3:]\n",
    "        pred[i] = models['%d'%i].predict(Xtr.reshape(1,-1))\n",
    "\n",
    "    Xtr = features_tr_new[\n",
    "                    np.union1d([],\n",
    "                        np.concatenate([features_tr.columns[linear_features[1]],\n",
    "                            features_tr.columns[linear_features[2]],\n",
    "                            features_tr.columns[linear_features[4]],\n",
    "                            features_tr.columns[linear_features[7]]])\n",
    "                    )\n",
    "                ].values[-3:]\n",
    "    pred[[1,2,4,7]] = models['1247'].predict(Xtr.reshape(1,-1))\n",
    "\n",
    "    return pred - A.values[-1]\n",
    "\n",
    "import critic \n",
    "cr = critic.Critic()\n",
    "cr.submit(get_r_hat, log_pr_tst, volu_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "48587160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.06726545e+14, 1.02180800e+14, 4.51339140e+13, 3.41720200e+13,\n",
       "       1.89815431e+13, 1.63715091e+13, 1.13642817e+13, 7.46451932e+12,\n",
       "       5.42882605e+12, 4.22126186e+12, 3.31065648e+12, 2.56829910e+12,\n",
       "       2.33181029e+12, 2.02342993e+12, 1.91278754e+12, 1.60096315e+12,\n",
       "       1.24399059e+12, 1.12374453e+12, 9.87104002e+11, 8.14410218e+11,\n",
       "       6.76718437e+11, 5.53467665e+11, 3.94622577e+11, 3.62120769e+11,\n",
       "       3.32416253e+11, 3.07339776e+11, 2.43945870e+11, 2.26553386e+11,\n",
       "       2.06957155e+11, 1.61014820e+11, 1.52393483e+11, 1.34079041e+11,\n",
       "       1.25317215e+11, 1.11744850e+11, 1.07693721e+11, 1.01965835e+11,\n",
       "       7.93070965e+10, 7.39840280e+10, 6.36544037e+10, 5.99292318e+10,\n",
       "       5.85220026e+10, 4.71637382e+10, 4.27089847e+10, 3.71181953e+10,\n",
       "       3.54575357e+10, 3.19856145e+10, 2.70406636e+10, 2.65227320e+10,\n",
       "       2.50154374e+10, 2.24553672e+10, 2.09505118e+10, 1.59031329e+10,\n",
       "       1.49945582e+10, 1.42952736e+10, 1.40530262e+10, 1.31710649e+10,\n",
       "       1.17229361e+10, 1.15231955e+10, 9.27180588e+09, 8.42210876e+09,\n",
       "       7.92787004e+09, 6.82716789e+09, 6.09152410e+09, 5.99901638e+09,\n",
       "       5.84704194e+09, 5.57941557e+09, 5.55807675e+09, 4.71433193e+09,\n",
       "       4.19815980e+09, 4.12020500e+09, 4.05040620e+09, 3.55252508e+09,\n",
       "       2.91637178e+09, 2.65898768e+09, 2.38603563e+09, 2.15766095e+09,\n",
       "       2.09258988e+09, 1.93197888e+09, 1.78023098e+09, 1.65152608e+09,\n",
       "       1.54943404e+09, 1.49353145e+09, 1.39690146e+09, 1.10356721e+09,\n",
       "       1.09411074e+09, 1.07186216e+09, 8.67323769e+08, 6.78492254e+08,\n",
       "       6.65157108e+08, 6.52483301e+08, 5.76132335e+08, 4.64730413e+08,\n",
       "       4.62088446e+08, 3.81670719e+08, 3.69020270e+08, 2.76449277e+08,\n",
       "       2.14629666e+08, 1.69749921e+08, 1.46101082e+08, 1.22394820e+08,\n",
       "       1.17064510e+08, 1.05122756e+08, 1.00017883e+08, 9.30302703e+07,\n",
       "       5.92684465e+07, 5.53514149e+07, 4.80146062e+07, 3.08610770e+07,\n",
       "       2.06557335e+07, 1.36308350e+07, 8.14446462e+06, 6.20742460e+06,\n",
       "       3.99764323e+06, 2.45929316e+06, 1.57327819e+06, 1.40615289e+06,\n",
       "       8.02598017e+05, 4.79125801e+05, 2.27124114e+05, 4.71158293e+04,\n",
       "       1.80259516e+03, 1.08465751e+03, 8.23428479e+02, 7.28235366e+02,\n",
       "       3.40571263e+02, 2.33701657e+02, 6.51678033e-01, 3.16031409e-01,\n",
       "       1.31813050e-01, 3.86531336e-02, 1.75808562e-02, 1.42739160e-02,\n",
       "       6.64565557e-03, 5.41060477e-03, 4.31875493e-03, 2.64107875e-03,\n",
       "       2.08991104e-03, 7.48216951e-04, 2.87359149e-04, 2.22588984e-04,\n",
       "       2.04271035e-04, 1.79479503e-04, 1.34719011e-04, 1.25794414e-04,\n",
       "       1.21114500e-04, 1.08285969e-04, 7.62845949e-05, 5.52727777e-05,\n",
       "       4.82421313e-05, 4.64765310e-05, 3.89898391e-05, 3.40222080e-05,\n",
       "       3.28657917e-05, 2.62097563e-05, 2.48920249e-05, 2.24738653e-05,\n",
       "       2.03942271e-05, 1.86508174e-05, 1.71321672e-05, 1.36771528e-05,\n",
       "       1.33269839e-05, 1.14218696e-05, 1.04964989e-05, 1.04244169e-05,\n",
       "       9.79610755e-06, 9.41278451e-06, 8.67903261e-06, 7.69801669e-06,\n",
       "       7.42816818e-06, 7.12149974e-06, 6.96863802e-06, 6.58655296e-06,\n",
       "       6.38479048e-06, 6.06741509e-06, 5.61137749e-06, 5.40618802e-06,\n",
       "       5.35309451e-06, 5.07722884e-06, 4.50898315e-06, 4.31894401e-06,\n",
       "       4.27959886e-06, 4.13495454e-06, 4.03763107e-06, 3.98928152e-06,\n",
       "       3.85506004e-06, 3.63869829e-06, 3.53623811e-06, 3.41048446e-06,\n",
       "       3.30870701e-06, 3.18327036e-06, 2.93702066e-06, 2.75293202e-06,\n",
       "       2.61854815e-06, 2.43491929e-06, 2.32892686e-06, 2.14260357e-06,\n",
       "       2.07962639e-06, 1.91319230e-06, 1.88742729e-06, 1.73336643e-06,\n",
       "       1.65932874e-06, 1.59709792e-06, 1.53027144e-06, 1.49635768e-06,\n",
       "       1.43009599e-06, 1.36508722e-06, 1.33344007e-06, 1.27757721e-06,\n",
       "       1.20516860e-06, 1.15857041e-06, 1.13592493e-06, 1.04358693e-06,\n",
       "       1.01974232e-06, 9.87543783e-07, 9.62927708e-07, 9.15541917e-07,\n",
       "       8.50522549e-07, 8.22995341e-07, 7.98589980e-07, 6.87045908e-07,\n",
       "       6.44681993e-07, 6.31435276e-07, 5.94770268e-07, 5.34058032e-07,\n",
       "       5.06382878e-07, 4.06906424e-07, 3.71448123e-07, 3.26049313e-07,\n",
       "       2.92080665e-07, 2.55569095e-07, 2.39532684e-07, 2.15541643e-07,\n",
       "       2.04284426e-07, 1.97963617e-07, 1.73967988e-07, 1.51763610e-07,\n",
       "       1.31819466e-07, 1.00663217e-07, 8.95660703e-08, 8.62574092e-08,\n",
       "       7.39222811e-08, 6.19489238e-08, 4.96374365e-08, 4.76477689e-08,\n",
       "       4.21810960e-08, 3.92873823e-08, 3.64730604e-08, 3.47639575e-08,\n",
       "       3.27844423e-08, 3.17029726e-08, 2.59801072e-08, 2.24090943e-08,\n",
       "       1.94843042e-08, 1.90740218e-08, 1.73393605e-08, 1.44256835e-08,\n",
       "       1.42153309e-08, 1.36905853e-08, 1.22554716e-08, 1.15803720e-08,\n",
       "       1.11061352e-08, 9.85629721e-09, 9.39337310e-09, 8.19321170e-09,\n",
       "       7.19560340e-09, 6.06956619e-09, 4.99011459e-09, 4.57479295e-09,\n",
       "       4.20613922e-09, 3.77165872e-09, 3.28907246e-09, 2.67494634e-09,\n",
       "       2.08315906e-09, 1.99307728e-09, 1.77273460e-09, 1.75285030e-09,\n",
       "       5.78537831e-10, 1.44504490e-10, 6.39092556e-11, 5.85618534e-11,\n",
       "       4.68662038e-11, 3.75879994e-11, 3.29411976e-11, 2.43925865e-11,\n",
       "       2.40032064e-11, 2.11863606e-11, 6.81753238e-12, 5.58064316e-18,\n",
       "       4.83830380e-18, 4.83830380e-18, 4.83830380e-18, 4.83830380e-18,\n",
       "       4.83830380e-18, 4.83830380e-18, 4.83830380e-18, 4.83830380e-18,\n",
       "       4.83830380e-18, 4.83830380e-18, 4.83830380e-18, 4.83830380e-18,\n",
       "       4.83830380e-18, 4.83830380e-18, 4.83830380e-18, 4.83830380e-18,\n",
       "       4.83830380e-18, 4.83830380e-18, 4.83830380e-18, 4.83830380e-18,\n",
       "       4.83830380e-18, 4.83830380e-18, 4.83830380e-18, 4.83830380e-18,\n",
       "       4.83830380e-18, 4.83830380e-18, 4.83830380e-18, 1.93743108e-18,\n",
       "       1.84354497e-19])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "features = features_tr_new.dropna()\n",
    "\n",
    "pca = PCA()\n",
    "components = pca.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ca5dc0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56], dtype=int64)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(pca.explained_variance_ratio_.cumsum() < 0.9999)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "c10f596f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prma100</th>\n",
       "      <th>prma101</th>\n",
       "      <th>prma102</th>\n",
       "      <th>prma103</th>\n",
       "      <th>prma104</th>\n",
       "      <th>prma105</th>\n",
       "      <th>prma106</th>\n",
       "      <th>prma107</th>\n",
       "      <th>prma108</th>\n",
       "      <th>prma109</th>\n",
       "      <th>prma300</th>\n",
       "      <th>prma301</th>\n",
       "      <th>prma302</th>\n",
       "      <th>prma303</th>\n",
       "      <th>prma304</th>\n",
       "      <th>prma305</th>\n",
       "      <th>prma306</th>\n",
       "      <th>prma307</th>\n",
       "      <th>prma308</th>\n",
       "      <th>prma309</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-07-01 00:29:00</th>\n",
       "      <td>-0.000602</td>\n",
       "      <td>0.002967</td>\n",
       "      <td>0.001147</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>-0.004269</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>-0.005965</td>\n",
       "      <td>-0.002026</td>\n",
       "      <td>0.002447</td>\n",
       "      <td>-0.000157</td>\n",
       "      <td>-0.000630</td>\n",
       "      <td>0.001905</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>-0.004880</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>-0.004987</td>\n",
       "      <td>-0.000422</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>-3.215239e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-01 00:30:00</th>\n",
       "      <td>-0.000691</td>\n",
       "      <td>0.002888</td>\n",
       "      <td>0.001268</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>-0.004405</td>\n",
       "      <td>0.002272</td>\n",
       "      <td>-0.006094</td>\n",
       "      <td>-0.002188</td>\n",
       "      <td>0.002475</td>\n",
       "      <td>-0.000158</td>\n",
       "      <td>-0.000686</td>\n",
       "      <td>0.002101</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>-0.005144</td>\n",
       "      <td>0.001570</td>\n",
       "      <td>-0.005167</td>\n",
       "      <td>-0.000516</td>\n",
       "      <td>0.002590</td>\n",
       "      <td>-7.159087e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-01 00:31:00</th>\n",
       "      <td>-0.000923</td>\n",
       "      <td>0.002824</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>-0.004549</td>\n",
       "      <td>0.002177</td>\n",
       "      <td>-0.006166</td>\n",
       "      <td>-0.002158</td>\n",
       "      <td>0.002487</td>\n",
       "      <td>-0.000188</td>\n",
       "      <td>-0.000762</td>\n",
       "      <td>0.002230</td>\n",
       "      <td>0.001282</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>-0.005339</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>-0.005284</td>\n",
       "      <td>-0.000610</td>\n",
       "      <td>0.002676</td>\n",
       "      <td>2.308114e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-01 00:32:00</th>\n",
       "      <td>-0.001233</td>\n",
       "      <td>0.002791</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>-0.004577</td>\n",
       "      <td>0.002125</td>\n",
       "      <td>-0.006159</td>\n",
       "      <td>-0.002123</td>\n",
       "      <td>0.002481</td>\n",
       "      <td>-0.000195</td>\n",
       "      <td>-0.000897</td>\n",
       "      <td>0.002281</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>-0.005396</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>-0.005298</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>0.002664</td>\n",
       "      <td>-1.380711e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-01 00:33:00</th>\n",
       "      <td>-0.001760</td>\n",
       "      <td>0.002805</td>\n",
       "      <td>0.001817</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>-0.004689</td>\n",
       "      <td>0.002085</td>\n",
       "      <td>-0.006188</td>\n",
       "      <td>-0.002179</td>\n",
       "      <td>0.002487</td>\n",
       "      <td>-0.000221</td>\n",
       "      <td>-0.001052</td>\n",
       "      <td>0.002339</td>\n",
       "      <td>0.001389</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>-0.005477</td>\n",
       "      <td>0.001726</td>\n",
       "      <td>-0.005402</td>\n",
       "      <td>-0.000779</td>\n",
       "      <td>0.002640</td>\n",
       "      <td>-3.018669e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-06 18:37:00</th>\n",
       "      <td>-0.309411</td>\n",
       "      <td>-0.709375</td>\n",
       "      <td>1.229860</td>\n",
       "      <td>0.380602</td>\n",
       "      <td>-0.368945</td>\n",
       "      <td>-0.220803</td>\n",
       "      <td>-0.119173</td>\n",
       "      <td>-0.220078</td>\n",
       "      <td>0.099622</td>\n",
       "      <td>-0.065734</td>\n",
       "      <td>-0.309082</td>\n",
       "      <td>-0.709848</td>\n",
       "      <td>1.230401</td>\n",
       "      <td>0.381596</td>\n",
       "      <td>-0.368693</td>\n",
       "      <td>-0.221218</td>\n",
       "      <td>-0.118665</td>\n",
       "      <td>-0.220254</td>\n",
       "      <td>0.098666</td>\n",
       "      <td>-6.603152e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-06 18:38:00</th>\n",
       "      <td>-0.309406</td>\n",
       "      <td>-0.709373</td>\n",
       "      <td>1.230079</td>\n",
       "      <td>0.380720</td>\n",
       "      <td>-0.368947</td>\n",
       "      <td>-0.220699</td>\n",
       "      <td>-0.119129</td>\n",
       "      <td>-0.219901</td>\n",
       "      <td>0.099753</td>\n",
       "      <td>-0.065589</td>\n",
       "      <td>-0.309089</td>\n",
       "      <td>-0.709810</td>\n",
       "      <td>1.230400</td>\n",
       "      <td>0.381541</td>\n",
       "      <td>-0.368744</td>\n",
       "      <td>-0.221180</td>\n",
       "      <td>-0.118696</td>\n",
       "      <td>-0.220186</td>\n",
       "      <td>0.098743</td>\n",
       "      <td>-6.599089e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-06 18:39:00</th>\n",
       "      <td>-0.309477</td>\n",
       "      <td>-0.709343</td>\n",
       "      <td>1.230210</td>\n",
       "      <td>0.380848</td>\n",
       "      <td>-0.368988</td>\n",
       "      <td>-0.220640</td>\n",
       "      <td>-0.119048</td>\n",
       "      <td>-0.219571</td>\n",
       "      <td>0.099831</td>\n",
       "      <td>-0.065478</td>\n",
       "      <td>-0.309089</td>\n",
       "      <td>-0.709775</td>\n",
       "      <td>1.230384</td>\n",
       "      <td>0.381505</td>\n",
       "      <td>-0.368806</td>\n",
       "      <td>-0.221134</td>\n",
       "      <td>-0.118718</td>\n",
       "      <td>-0.220080</td>\n",
       "      <td>0.098803</td>\n",
       "      <td>-6.595308e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-06 18:40:00</th>\n",
       "      <td>-0.309470</td>\n",
       "      <td>-0.709336</td>\n",
       "      <td>1.230382</td>\n",
       "      <td>0.380943</td>\n",
       "      <td>-0.369079</td>\n",
       "      <td>-0.220531</td>\n",
       "      <td>-0.118987</td>\n",
       "      <td>-0.219271</td>\n",
       "      <td>0.099904</td>\n",
       "      <td>-0.065363</td>\n",
       "      <td>-0.309089</td>\n",
       "      <td>-0.709731</td>\n",
       "      <td>1.230339</td>\n",
       "      <td>0.381450</td>\n",
       "      <td>-0.368857</td>\n",
       "      <td>-0.221072</td>\n",
       "      <td>-0.118755</td>\n",
       "      <td>-0.219964</td>\n",
       "      <td>0.098874</td>\n",
       "      <td>-6.590293e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-06 18:41:00</th>\n",
       "      <td>-0.309302</td>\n",
       "      <td>-0.709427</td>\n",
       "      <td>1.230536</td>\n",
       "      <td>0.380986</td>\n",
       "      <td>-0.369160</td>\n",
       "      <td>-0.220509</td>\n",
       "      <td>-0.118900</td>\n",
       "      <td>-0.218891</td>\n",
       "      <td>0.099875</td>\n",
       "      <td>-0.065265</td>\n",
       "      <td>-0.309061</td>\n",
       "      <td>-0.709734</td>\n",
       "      <td>1.230186</td>\n",
       "      <td>0.381409</td>\n",
       "      <td>-0.368900</td>\n",
       "      <td>-0.221042</td>\n",
       "      <td>-0.118762</td>\n",
       "      <td>-0.219837</td>\n",
       "      <td>0.098934</td>\n",
       "      <td>-6.585169e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>185413 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      prma100   prma101   prma102   prma103   prma104  \\\n",
       "timestamp                                                               \n",
       "2021-07-01 00:29:00 -0.000602  0.002967  0.001147  0.000601 -0.004269   \n",
       "2021-07-01 00:30:00 -0.000691  0.002888  0.001268  0.000758 -0.004405   \n",
       "2021-07-01 00:31:00 -0.000923  0.002824  0.001392  0.000981 -0.004549   \n",
       "2021-07-01 00:32:00 -0.001233  0.002791  0.001561  0.001383 -0.004577   \n",
       "2021-07-01 00:33:00 -0.001760  0.002805  0.001817  0.001801 -0.004689   \n",
       "...                       ...       ...       ...       ...       ...   \n",
       "2021-11-06 18:37:00 -0.309411 -0.709375  1.229860  0.380602 -0.368945   \n",
       "2021-11-06 18:38:00 -0.309406 -0.709373  1.230079  0.380720 -0.368947   \n",
       "2021-11-06 18:39:00 -0.309477 -0.709343  1.230210  0.380848 -0.368988   \n",
       "2021-11-06 18:40:00 -0.309470 -0.709336  1.230382  0.380943 -0.369079   \n",
       "2021-11-06 18:41:00 -0.309302 -0.709427  1.230536  0.380986 -0.369160   \n",
       "\n",
       "                      prma105   prma106   prma107   prma108   prma109  \\\n",
       "timestamp                                                               \n",
       "2021-07-01 00:29:00  0.002300 -0.005965 -0.002026  0.002447 -0.000157   \n",
       "2021-07-01 00:30:00  0.002272 -0.006094 -0.002188  0.002475 -0.000158   \n",
       "2021-07-01 00:31:00  0.002177 -0.006166 -0.002158  0.002487 -0.000188   \n",
       "2021-07-01 00:32:00  0.002125 -0.006159 -0.002123  0.002481 -0.000195   \n",
       "2021-07-01 00:33:00  0.002085 -0.006188 -0.002179  0.002487 -0.000221   \n",
       "...                       ...       ...       ...       ...       ...   \n",
       "2021-11-06 18:37:00 -0.220803 -0.119173 -0.220078  0.099622 -0.065734   \n",
       "2021-11-06 18:38:00 -0.220699 -0.119129 -0.219901  0.099753 -0.065589   \n",
       "2021-11-06 18:39:00 -0.220640 -0.119048 -0.219571  0.099831 -0.065478   \n",
       "2021-11-06 18:40:00 -0.220531 -0.118987 -0.219271  0.099904 -0.065363   \n",
       "2021-11-06 18:41:00 -0.220509 -0.118900 -0.218891  0.099875 -0.065265   \n",
       "\n",
       "                      prma300   prma301   prma302   prma303   prma304  \\\n",
       "timestamp                                                               \n",
       "2021-07-01 00:29:00 -0.000630  0.001905  0.001071  0.000138 -0.004880   \n",
       "2021-07-01 00:30:00 -0.000686  0.002101  0.001220  0.000225 -0.005144   \n",
       "2021-07-01 00:31:00 -0.000762  0.002230  0.001282  0.000311 -0.005339   \n",
       "2021-07-01 00:32:00 -0.000897  0.002281  0.001329  0.000459 -0.005396   \n",
       "2021-07-01 00:33:00 -0.001052  0.002339  0.001389  0.000629 -0.005477   \n",
       "...                       ...       ...       ...       ...       ...   \n",
       "2021-11-06 18:37:00 -0.309082 -0.709848  1.230401  0.381596 -0.368693   \n",
       "2021-11-06 18:38:00 -0.309089 -0.709810  1.230400  0.381541 -0.368744   \n",
       "2021-11-06 18:39:00 -0.309089 -0.709775  1.230384  0.381505 -0.368806   \n",
       "2021-11-06 18:40:00 -0.309089 -0.709731  1.230339  0.381450 -0.368857   \n",
       "2021-11-06 18:41:00 -0.309061 -0.709734  1.230186  0.381409 -0.368900   \n",
       "\n",
       "                      prma305   prma306   prma307   prma308       prma309  \n",
       "timestamp                                                                  \n",
       "2021-07-01 00:29:00  0.001506 -0.004987 -0.000422  0.002465 -3.215239e-05  \n",
       "2021-07-01 00:30:00  0.001570 -0.005167 -0.000516  0.002590 -7.159087e-06  \n",
       "2021-07-01 00:31:00  0.001629 -0.005284 -0.000610  0.002676  2.308114e-07  \n",
       "2021-07-01 00:32:00  0.001674 -0.005298 -0.000681  0.002664 -1.380711e-05  \n",
       "2021-07-01 00:33:00  0.001726 -0.005402 -0.000779  0.002640 -3.018669e-05  \n",
       "...                       ...       ...       ...       ...           ...  \n",
       "2021-11-06 18:37:00 -0.221218 -0.118665 -0.220254  0.098666 -6.603152e-02  \n",
       "2021-11-06 18:38:00 -0.221180 -0.118696 -0.220186  0.098743 -6.599089e-02  \n",
       "2021-11-06 18:39:00 -0.221134 -0.118718 -0.220080  0.098803 -6.595308e-02  \n",
       "2021-11-06 18:40:00 -0.221072 -0.118755 -0.219964  0.098874 -6.590293e-02  \n",
       "2021-11-06 18:41:00 -0.221042 -0.118762 -0.219837  0.098934 -6.585169e-02  \n",
       "\n",
       "[185413 rows x 20 columns]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = features_tr_new[[\"prma%d%d\"%(i,j) for i in [10, 30] for j in range(10)]].dropna().iloc[:-30]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "c3c0c974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_pr_0</th>\n",
       "      <th>log_pr_1</th>\n",
       "      <th>log_pr_2</th>\n",
       "      <th>log_pr_3</th>\n",
       "      <th>log_pr_4</th>\n",
       "      <th>log_pr_5</th>\n",
       "      <th>log_pr_6</th>\n",
       "      <th>log_pr_7</th>\n",
       "      <th>log_pr_8</th>\n",
       "      <th>log_pr_9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-07-01 00:29:00</th>\n",
       "      <td>-0.009531</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.004872</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>-0.001679</td>\n",
       "      <td>0.002308</td>\n",
       "      <td>-0.005915</td>\n",
       "      <td>-0.000483</td>\n",
       "      <td>0.002788</td>\n",
       "      <td>0.003949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-01 00:30:00</th>\n",
       "      <td>-0.010765</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.004830</td>\n",
       "      <td>0.001079</td>\n",
       "      <td>-0.001231</td>\n",
       "      <td>0.002444</td>\n",
       "      <td>-0.005569</td>\n",
       "      <td>-0.000354</td>\n",
       "      <td>0.002661</td>\n",
       "      <td>0.003648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-01 00:31:00</th>\n",
       "      <td>-0.009597</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.004416</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>-0.001727</td>\n",
       "      <td>0.002139</td>\n",
       "      <td>-0.005591</td>\n",
       "      <td>-0.000934</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>0.003161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-01 00:32:00</th>\n",
       "      <td>-0.009994</td>\n",
       "      <td>-0.000104</td>\n",
       "      <td>0.005678</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>-0.002066</td>\n",
       "      <td>0.002131</td>\n",
       "      <td>-0.005633</td>\n",
       "      <td>-0.001691</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.002403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-01 00:33:00</th>\n",
       "      <td>-0.010669</td>\n",
       "      <td>-0.000924</td>\n",
       "      <td>0.006833</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>-0.002008</td>\n",
       "      <td>0.002030</td>\n",
       "      <td>-0.004733</td>\n",
       "      <td>-0.001976</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.002614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-06 18:37:00</th>\n",
       "      <td>-0.311879</td>\n",
       "      <td>-0.713263</td>\n",
       "      <td>1.231817</td>\n",
       "      <td>0.380252</td>\n",
       "      <td>-0.368900</td>\n",
       "      <td>-0.221951</td>\n",
       "      <td>-0.116951</td>\n",
       "      <td>-0.217529</td>\n",
       "      <td>0.099292</td>\n",
       "      <td>-0.065818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-06 18:38:00</th>\n",
       "      <td>-0.311817</td>\n",
       "      <td>-0.713298</td>\n",
       "      <td>1.231063</td>\n",
       "      <td>0.380303</td>\n",
       "      <td>-0.367879</td>\n",
       "      <td>-0.221343</td>\n",
       "      <td>-0.116840</td>\n",
       "      <td>-0.219413</td>\n",
       "      <td>0.099787</td>\n",
       "      <td>-0.065844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-06 18:39:00</th>\n",
       "      <td>-0.312098</td>\n",
       "      <td>-0.713280</td>\n",
       "      <td>1.230117</td>\n",
       "      <td>0.379512</td>\n",
       "      <td>-0.368103</td>\n",
       "      <td>-0.221024</td>\n",
       "      <td>-0.116074</td>\n",
       "      <td>-0.219266</td>\n",
       "      <td>0.100709</td>\n",
       "      <td>-0.065608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-06 18:40:00</th>\n",
       "      <td>-0.312339</td>\n",
       "      <td>-0.712533</td>\n",
       "      <td>1.229353</td>\n",
       "      <td>0.380235</td>\n",
       "      <td>-0.368684</td>\n",
       "      <td>-0.220570</td>\n",
       "      <td>-0.116050</td>\n",
       "      <td>-0.221033</td>\n",
       "      <td>0.100910</td>\n",
       "      <td>-0.065666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-06 18:41:00</th>\n",
       "      <td>-0.312399</td>\n",
       "      <td>-0.712926</td>\n",
       "      <td>1.230809</td>\n",
       "      <td>0.380299</td>\n",
       "      <td>-0.368836</td>\n",
       "      <td>-0.220758</td>\n",
       "      <td>-0.116377</td>\n",
       "      <td>-0.221310</td>\n",
       "      <td>0.101354</td>\n",
       "      <td>-0.065411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>185413 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     log_pr_0  log_pr_1  log_pr_2  log_pr_3  log_pr_4  \\\n",
       "timestamp                                                               \n",
       "2021-07-01 00:29:00 -0.009531  0.000080  0.004872  0.001754 -0.001679   \n",
       "2021-07-01 00:30:00 -0.010765  0.001015  0.004830  0.001079 -0.001231   \n",
       "2021-07-01 00:31:00 -0.009597  0.001311  0.004416  0.001300 -0.001727   \n",
       "2021-07-01 00:32:00 -0.009994 -0.000104  0.005678  0.000674 -0.002066   \n",
       "2021-07-01 00:33:00 -0.010669 -0.000924  0.006833  0.000172 -0.002008   \n",
       "...                       ...       ...       ...       ...       ...   \n",
       "2021-11-06 18:37:00 -0.311879 -0.713263  1.231817  0.380252 -0.368900   \n",
       "2021-11-06 18:38:00 -0.311817 -0.713298  1.231063  0.380303 -0.367879   \n",
       "2021-11-06 18:39:00 -0.312098 -0.713280  1.230117  0.379512 -0.368103   \n",
       "2021-11-06 18:40:00 -0.312339 -0.712533  1.229353  0.380235 -0.368684   \n",
       "2021-11-06 18:41:00 -0.312399 -0.712926  1.230809  0.380299 -0.368836   \n",
       "\n",
       "                     log_pr_5  log_pr_6  log_pr_7  log_pr_8  log_pr_9  \n",
       "timestamp                                                              \n",
       "2021-07-01 00:29:00  0.002308 -0.005915 -0.000483  0.002788  0.003949  \n",
       "2021-07-01 00:30:00  0.002444 -0.005569 -0.000354  0.002661  0.003648  \n",
       "2021-07-01 00:31:00  0.002139 -0.005591 -0.000934  0.002016  0.003161  \n",
       "2021-07-01 00:32:00  0.002131 -0.005633 -0.001691  0.000854  0.002403  \n",
       "2021-07-01 00:33:00  0.002030 -0.004733 -0.001976  0.000409  0.002614  \n",
       "...                       ...       ...       ...       ...       ...  \n",
       "2021-11-06 18:37:00 -0.221951 -0.116951 -0.217529  0.099292 -0.065818  \n",
       "2021-11-06 18:38:00 -0.221343 -0.116840 -0.219413  0.099787 -0.065844  \n",
       "2021-11-06 18:39:00 -0.221024 -0.116074 -0.219266  0.100709 -0.065608  \n",
       "2021-11-06 18:40:00 -0.220570 -0.116050 -0.221033  0.100910 -0.065666  \n",
       "2021-11-06 18:41:00 -0.220758 -0.116377 -0.221310  0.101354 -0.065411  \n",
       "\n",
       "[185413 rows x 10 columns]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = log_pr_tr.shift(-30).loc[X.index]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "b6f50ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for i in range(10):\n",
    "    models.append(LinearRegression().fit(X[[\"prma10%d\"%i, \"prma30%d\"%i]], y.iloc[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "0fabbf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7805 [00:00<?, ?it/s]d:\\UserProjects\\stats601-project\\.venv\\lib\\site-packages\\sklearn\\base.py:450: UserWarning:\n",
      "\n",
      "X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "\n",
      "  0%|          | 0/7805 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[-0.3361007  -0.33602037].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\UserProjects\\stats601-project\\data_generator.ipynb Cell 78'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/UserProjects/stats601-project/data_generator.ipynb#ch0000092?line=5'>6</a>\u001b[0m         pred[i] \u001b[39m=\u001b[39m models[i]\u001b[39m.\u001b[39mpredict([ma10[i], ma30[i]])\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/UserProjects/stats601-project/data_generator.ipynb#ch0000092?line=6'>7</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m pred \u001b[39m-\u001b[39m A\u001b[39m.\u001b[39mvalues[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/UserProjects/stats601-project/data_generator.ipynb#ch0000092?line=8'>9</a>\u001b[0m cr\u001b[39m.\u001b[39;49msubmit(get_r_hat, log_pr_tst, volu_tst)\n",
      "File \u001b[1;32md:\\UserProjects\\stats601-project\\critic.py:73\u001b[0m, in \u001b[0;36mCritic.submit\u001b[1;34m(self, get_r_hat, log_pr, volu)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/UserProjects/stats601-project/critic.py?line=71'>72</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msubmit\u001b[39m(\u001b[39mself\u001b[39m, get_r_hat, log_pr, volu):\n\u001b[1;32m---> <a href='file:///d%3A/UserProjects/stats601-project/critic.py?line=72'>73</a>\u001b[0m     t_used, pairwise, overall \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscore(get_r_hat, log_pr, volu)\n\u001b[0;32m     <a href='file:///d%3A/UserProjects/stats601-project/critic.py?line=73'>74</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTotal time used: \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39ms\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m t_used)\n\u001b[0;32m     <a href='file:///d%3A/UserProjects/stats601-project/critic.py?line=74'>75</a>\u001b[0m     pairwise_report \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPairwise correlation:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32md:\\UserProjects\\stats601-project\\critic.py:51\u001b[0m, in \u001b[0;36mCritic.score\u001b[1;34m(self, get_r_hat, log_pr, volu)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/UserProjects/stats601-project/critic.py?line=46'>47</a>\u001b[0m r_hat \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(index\u001b[39m=\u001b[39mlog_pr\u001b[39m.\u001b[39mindex[\u001b[39m1440\u001b[39m::\u001b[39m10\u001b[39m], \n\u001b[0;32m     <a href='file:///d%3A/UserProjects/stats601-project/critic.py?line=47'>48</a>\u001b[0m                     columns\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39marange(\u001b[39m10\u001b[39m), \n\u001b[0;32m     <a href='file:///d%3A/UserProjects/stats601-project/critic.py?line=48'>49</a>\u001b[0m                     dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64)\n\u001b[0;32m     <a href='file:///d%3A/UserProjects/stats601-project/critic.py?line=49'>50</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m tqdm(log_pr\u001b[39m.\u001b[39mindex[\u001b[39m1440\u001b[39m::\u001b[39m10\u001b[39m]): \u001b[39m# compute the predictions every day\u001b[39;00m\n\u001b[1;32m---> <a href='file:///d%3A/UserProjects/stats601-project/critic.py?line=50'>51</a>\u001b[0m     r_hat\u001b[39m.\u001b[39mloc[t, :] \u001b[39m=\u001b[39m get_r_hat(log_pr\u001b[39m.\u001b[39;49mloc[(t \u001b[39m-\u001b[39;49m dt):t], volu\u001b[39m.\u001b[39;49mloc[(t \u001b[39m-\u001b[39;49m dt):t])\n\u001b[0;32m     <a href='file:///d%3A/UserProjects/stats601-project/critic.py?line=52'>53</a>\u001b[0m t_used \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[0;32m     <a href='file:///d%3A/UserProjects/stats601-project/critic.py?line=54'>55</a>\u001b[0m \u001b[39m# Compute true forward log_returns every 10 minutes\u001b[39;00m\n",
      "\u001b[1;32md:\\UserProjects\\stats601-project\\data_generator.ipynb Cell 78'\u001b[0m in \u001b[0;36mget_r_hat\u001b[1;34m(A, B)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/UserProjects/stats601-project/data_generator.ipynb#ch0000092?line=3'>4</a>\u001b[0m pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39m10\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/UserProjects/stats601-project/data_generator.ipynb#ch0000092?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/UserProjects/stats601-project/data_generator.ipynb#ch0000092?line=5'>6</a>\u001b[0m     pred[i] \u001b[39m=\u001b[39m models[i]\u001b[39m.\u001b[39;49mpredict([ma10[i], ma30[i]])\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/UserProjects/stats601-project/data_generator.ipynb#ch0000092?line=6'>7</a>\u001b[0m \u001b[39mreturn\u001b[39;00m pred \u001b[39m-\u001b[39m A\u001b[39m.\u001b[39mvalues[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32md:\\UserProjects\\stats601-project\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_base.py:362\u001b[0m, in \u001b[0;36mLinearModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/UserProjects/stats601-project/.venv/lib/site-packages/sklearn/linear_model/_base.py?line=347'>348</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    <a href='file:///d%3A/UserProjects/stats601-project/.venv/lib/site-packages/sklearn/linear_model/_base.py?line=348'>349</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/UserProjects/stats601-project/.venv/lib/site-packages/sklearn/linear_model/_base.py?line=349'>350</a>\u001b[0m \u001b[39m    Predict using the linear model.\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/UserProjects/stats601-project/.venv/lib/site-packages/sklearn/linear_model/_base.py?line=350'>351</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/UserProjects/stats601-project/.venv/lib/site-packages/sklearn/linear_model/_base.py?line=359'>360</a>\u001b[0m \u001b[39m        Returns predicted values.\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/UserProjects/stats601-project/.venv/lib/site-packages/sklearn/linear_model/_base.py?line=360'>361</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/UserProjects/stats601-project/.venv/lib/site-packages/sklearn/linear_model/_base.py?line=361'>362</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decision_function(X)\n",
      "File \u001b[1;32md:\\UserProjects\\stats601-project\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_base.py:345\u001b[0m, in \u001b[0;36mLinearModel._decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/UserProjects/stats601-project/.venv/lib/site-packages/sklearn/linear_model/_base.py?line=341'>342</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decision_function\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    <a href='file:///d%3A/UserProjects/stats601-project/.venv/lib/site-packages/sklearn/linear_model/_base.py?line=342'>343</a>\u001b[0m     check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> <a href='file:///d%3A/UserProjects/stats601-project/.venv/lib/site-packages/sklearn/linear_model/_base.py?line=344'>345</a>\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, accept_sparse\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcoo\u001b[39;49m\u001b[39m\"\u001b[39;49m], reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    <a href='file:///d%3A/UserProjects/stats601-project/.venv/lib/site-packages/sklearn/linear_model/_base.py?line=345'>346</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_\u001b[39m.\u001b[39mT, dense_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_\n",
      "File \u001b[1;32md:\\UserProjects\\stats601-project\\.venv\\lib\\site-packages\\sklearn\\base.py:566\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/UserProjects/stats601-project/.venv/lib/site-packages/sklearn/base.py?line=563'>564</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///d%3A/UserProjects/stats601-project/.venv/lib/site-packages/sklearn/base.py?line=564'>565</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> <a href='file:///d%3A/UserProjects/stats601-project/.venv/lib/site-packages/sklearn/base.py?line=565'>566</a>\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    <a href='file:///d%3A/UserProjects/stats601-project/.venv/lib/site-packages/sklearn/base.py?line=566'>567</a>\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    <a href='file:///d%3A/UserProjects/stats601-project/.venv/lib/site-packages/sklearn/base.py?line=567'>568</a>\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32md:\\UserProjects\\stats601-project\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:769\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/UserProjects/stats601-project/.venv/lib/site-packages/sklearn/utils/validation.py?line=766'>767</a>\u001b[0m     \u001b[39m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/UserProjects/stats601-project/.venv/lib/site-packages/sklearn/utils/validation.py?line=767'>768</a>\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> <a href='file:///d%3A/UserProjects/stats601-project/.venv/lib/site-packages/sklearn/utils/validation.py?line=768'>769</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///d%3A/UserProjects/stats601-project/.venv/lib/site-packages/sklearn/utils/validation.py?line=769'>770</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39marray=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///d%3A/UserProjects/stats601-project/.venv/lib/site-packages/sklearn/utils/validation.py?line=770'>771</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///d%3A/UserProjects/stats601-project/.venv/lib/site-packages/sklearn/utils/validation.py?line=771'>772</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///d%3A/UserProjects/stats601-project/.venv/lib/site-packages/sklearn/utils/validation.py?line=772'>773</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mif it contains a single sample.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    <a href='file:///d%3A/UserProjects/stats601-project/.venv/lib/site-packages/sklearn/utils/validation.py?line=773'>774</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///d%3A/UserProjects/stats601-project/.venv/lib/site-packages/sklearn/utils/validation.py?line=775'>776</a>\u001b[0m \u001b[39m# make sure we actually converted to numeric:\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/UserProjects/stats601-project/.venv/lib/site-packages/sklearn/utils/validation.py?line=776'>777</a>\u001b[0m \u001b[39mif\u001b[39;00m dtype_numeric \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mOUSV\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[-0.3361007  -0.33602037].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "def get_r_hat(A, B):\n",
    "    ma10 = A.rolling(10).mean().iloc[-1]\n",
    "    ma30 = A.rolling(30).mean().iloc[-1]\n",
    "    pred = np.zeros(10)\n",
    "    for i in range(10):\n",
    "        pred[i] = models[i].predict([[ma10[i], ma30[i]])\n",
    "    return pred - A.values[-1]\n",
    "\n",
    "cr.submit(get_r_hat, log_pr_tst, volu_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "e72bd3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7805/7805 [00:11<00:00, 675.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time used: 11.558s\n",
      "Pairwise correlation:\n",
      "\tasset 0 = 0.03150\n",
      "\tasset 1 = 0.10723\n",
      "\tasset 2 = 0.00321\n",
      "\tasset 3 = 0.03563\n",
      "\tasset 4 = -0.00836\n",
      "\tasset 5 = -0.00343\n",
      "\tasset 6 = 0.01458\n",
      "\tasset 7 = 0.01020\n",
      "\tasset 8 = 0.00271\n",
      "\tasset 9 = -0.02898\n",
      "\tmean correlation = 0.01643\n",
      "Overall correlation: 0.02432\n",
      "===============================\n",
      "Fail to outperform Ziwei's method, whose pairwise average\n",
      "and overall correlations are (0.02840, 0.01536)\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11.558045625686646,\n",
       " 0    0.031497\n",
       " 1    0.107226\n",
       " 2    0.003214\n",
       " 3    0.035634\n",
       " 4   -0.008358\n",
       " 5   -0.003428\n",
       " 6    0.014584\n",
       " 7    0.010199\n",
       " 8    0.002706\n",
       " 9   -0.028980\n",
       " dtype: float64,\n",
       " 0.02431859106970713)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_r_hat(A, B):\n",
    "    ma = A.rolling(10).mean()\n",
    "    ema = ma.ewm(com = 10 - 1, adjust=True, min_periods = 10).mean().values\n",
    "    return ma.values[-1] - A.values[-1]\n",
    "\n",
    "cr.submit(get_r_hat, log_pr_tst, volu_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773fcf24",
   "metadata": {},
   "source": [
    "#### PCA and Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb9a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal pca on linear features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e48dbda",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24df1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0057           0.0012            0.17s\n",
      "         2           0.0047           0.0010            0.16s\n",
      "         3           0.0039           0.0009            0.15s\n",
      "         4           0.0032           0.0007            0.14s\n",
      "         5           0.0026           0.0006            0.13s\n",
      "         6           0.0021           0.0004            0.12s\n",
      "         7           0.0018           0.0004            0.11s\n",
      "         8           0.0014           0.0003            0.10s\n",
      "         9           0.0013           0.0002            0.09s\n",
      "        10           0.0010           0.0002            0.09s\n",
      "        20           0.0002           0.0000            0.00s\n",
      "model 0 score = 0.95\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0108           0.0022            0.17s\n",
      "         2           0.0092           0.0017            0.15s\n",
      "         3           0.0077           0.0015            0.14s\n",
      "         4           0.0071           0.0012            0.13s\n",
      "         5           0.0057           0.0010            0.12s\n",
      "         6           0.0047           0.0009            0.11s\n",
      "         7           0.0041           0.0007            0.11s\n",
      "         8           0.0036           0.0007            0.10s\n",
      "         9           0.0030           0.0005            0.09s\n",
      "        10           0.0025           0.0005            0.08s\n",
      "        20           0.0006           0.0001            0.00s\n",
      "model 2 score = 0.94\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0051           0.0011            0.17s\n",
      "         2           0.0045           0.0009            0.16s\n",
      "         3           0.0037           0.0007            0.14s\n",
      "         4           0.0031           0.0006            0.14s\n",
      "         5           0.0026           0.0005            0.13s\n",
      "         6           0.0022           0.0004            0.12s\n",
      "         7           0.0019           0.0003            0.11s\n",
      "         8           0.0015           0.0003            0.10s\n",
      "         9           0.0013           0.0002            0.09s\n",
      "        10           0.0011           0.0002            0.08s\n",
      "        20           0.0002           0.0000            0.00s\n",
      "model 3 score = 0.96\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0019           0.0004            0.19s\n",
      "         2           0.0016           0.0003            0.16s\n",
      "         3           0.0013           0.0003            0.15s\n",
      "         4           0.0011           0.0002            0.14s\n",
      "         5           0.0009           0.0002            0.13s\n",
      "         6           0.0008           0.0002            0.12s\n",
      "         7           0.0006           0.0001            0.11s\n",
      "         8           0.0005           0.0001            0.10s\n",
      "         9           0.0004           0.0001            0.09s\n",
      "        10           0.0003           0.0001            0.08s\n",
      "        20           0.0001           0.0000            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0024           0.0005            0.17s\n",
      "         2           0.0019           0.0004            0.16s\n",
      "         3           0.0017           0.0004            0.15s\n",
      "         4           0.0014           0.0003            0.14s\n",
      "         5           0.0011           0.0002            0.13s\n",
      "         6           0.0010           0.0002            0.12s\n",
      "         7           0.0008           0.0002            0.11s\n",
      "         8           0.0006           0.0001            0.10s\n",
      "         9           0.0005           0.0001            0.09s\n",
      "        10           0.0004           0.0001            0.09s\n",
      "        20           0.0001           0.0000            0.00s\n",
      "model 8,9 score = 0.97\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0009           0.0001            0.20s\n",
      "         2           0.0007           0.0001            0.20s\n",
      "         3           0.0006           0.0001            0.18s\n",
      "         4           0.0005           0.0001            0.16s\n",
      "         5           0.0004           0.0001            0.15s\n",
      "         6           0.0004           0.0001            0.14s\n",
      "         7           0.0004           0.0001            0.13s\n",
      "         8           0.0003           0.0000            0.12s\n",
      "         9           0.0003           0.0000            0.11s\n",
      "        10           0.0002           0.0000            0.10s\n",
      "        20           0.0001           0.0000            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0007           0.0001            0.19s\n",
      "         2           0.0006           0.0001            0.18s\n",
      "         3           0.0005           0.0001            0.17s\n",
      "         4           0.0004           0.0001            0.16s\n",
      "         5           0.0004           0.0001            0.15s\n",
      "         6           0.0003           0.0001            0.14s\n",
      "         7           0.0003           0.0000            0.13s\n",
      "         8           0.0002           0.0000            0.12s\n",
      "         9           0.0002           0.0000            0.10s\n",
      "        10           0.0002           0.0000            0.09s\n",
      "        20           0.0000           0.0000            0.00s\n",
      "model 1,6 score = 0.91\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0015           0.0003            0.15s\n",
      "         2           0.0012           0.0002            0.15s\n",
      "         3           0.0010           0.0002            0.14s\n",
      "         4           0.0009           0.0001            0.13s\n",
      "         5           0.0007           0.0001            0.12s\n",
      "         6           0.0006           0.0001            0.12s\n",
      "         7           0.0005           0.0001            0.11s\n",
      "         8           0.0004           0.0001            0.10s\n",
      "         9           0.0004           0.0001            0.09s\n",
      "        10           0.0003           0.0001            0.08s\n",
      "        20           0.0001           0.0000            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0057           0.0012            0.18s\n",
      "         2           0.0049           0.0010            0.16s\n",
      "         3           0.0040           0.0008            0.15s\n",
      "         4           0.0034           0.0007            0.14s\n",
      "         5           0.0028           0.0006            0.13s\n",
      "         6           0.0023           0.0005            0.12s\n",
      "         7           0.0020           0.0004            0.11s\n",
      "         8           0.0017           0.0003            0.10s\n",
      "         9           0.0014           0.0003            0.09s\n",
      "        10           0.0011           0.0002            0.08s\n",
      "        20           0.0002           0.0000            0.00s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0005           0.0000            0.17s\n",
      "         2           0.0005           0.0001            0.16s\n",
      "         3           0.0004           0.0001            0.15s\n",
      "         4           0.0003           0.0000            0.14s\n",
      "         5           0.0003           0.0000            0.13s\n",
      "         6           0.0003           0.0000            0.11s\n",
      "         7           0.0002           0.0000            0.11s\n",
      "         8           0.0002           0.0000            0.10s\n",
      "         9           0.0002           0.0000            0.09s\n",
      "        10           0.0002           0.0000            0.08s\n",
      "        20           0.0001           0.0000            0.00s\n",
      "model 4,5,7 score = 0.93\n"
     ]
    }
   ],
   "source": [
    "# define \n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "models = {}\n",
    "params = dict(loss='huber', \n",
    "            n_estimators=20, \n",
    "            subsample=0.4, \n",
    "            max_depth=2, \n",
    "            min_samples_leaf=100,\n",
    "            max_features='log2',\n",
    "            verbose=1,\n",
    "            n_iter_no_change=10\n",
    "            )\n",
    "\n",
    "\n",
    "fname0 = np.union1d(selected_cor[0], selected_mi[0])\n",
    "fname0 = features_tr.columns[fname0]\n",
    "\n",
    "features_tr_f_new, label_tr_f_new = formulize_data(\n",
    "                features_tr_new.dropna()[fname0].iloc[-1440*60:], \n",
    "                log_pr_tr.loc[features_tr_new.dropna()[fname0].iloc[-1440*60:].index], \n",
    "                window_size=3)\n",
    "\n",
    "N = len(features_tr_f_new)\n",
    "\n",
    "models['model0'] = GradientBoostingRegressor(**params).fit(features_tr_f_new.reshape(N,-1), label_tr_f_new.squeeze()[:,0])\n",
    "print('model 0 score = %.2f'%models['model0'].score(features_tr_f_new.reshape(N,-1), label_tr_f_new.squeeze()[:,0]))\n",
    "\n",
    "\n",
    "\n",
    "fname2 = np.union1d(selected_cor[2], selected_mi[2])\n",
    "fname2 = features_tr.columns[fname2]\n",
    "\n",
    "features_tr_f_new, label_tr_f_new = formulize_data(\n",
    "                features_tr_new.dropna()[fname2].iloc[-1440*60:], \n",
    "                log_pr_tr.loc[features_tr_new.dropna()[fname2].iloc[-1440*60:].index], \n",
    "                window_size=3)\n",
    "\n",
    "models['model2'] = GradientBoostingRegressor(**params).fit(features_tr_f_new.reshape(N,-1), label_tr_f_new.squeeze()[:,2])\n",
    "print('model 2 score = %.2f'%models['model2'].score(features_tr_f_new.reshape(N,-1), label_tr_f_new.squeeze()[:,2]))\n",
    "\n",
    "\n",
    "\n",
    "fname3 = np.union1d(selected_cor[3], selected_mi[3])\n",
    "fname3 = features_tr.columns[fname3]\n",
    "\n",
    "features_tr_f_new, label_tr_f_new = formulize_data(\n",
    "                features_tr_new[fname3].dropna().iloc[-1440*60:], \n",
    "                log_pr_tr.loc[features_tr_new[fname3].dropna().iloc[-1440*60:].index], \n",
    "                window_size=3)\n",
    "\n",
    "models['model3'] = GradientBoostingRegressor(**params).fit(features_tr_f_new.reshape(N,-1), label_tr_f_new.squeeze()[:,3])\n",
    "print('model 3 score = %.2f'%models['model3'].score(features_tr_f_new.reshape(N,-1), label_tr_f_new.squeeze()[:,3]))\n",
    "\n",
    "\n",
    "\n",
    "fname8 = np.union1d(selected_cor[8], selected_mi[8])\n",
    "fname8 = features_tr.columns[fname8]\n",
    "fname9 = np.union1d(selected_cor[9], selected_mi[9])\n",
    "fname9 = features_tr.columns[fname9]\n",
    "fname89 = np.union1d(fname8, fname9)\n",
    "\n",
    "features_tr_f_new, label_tr_f_new = formulize_data(\n",
    "                features_tr_new.dropna()[fname89].iloc[-1440*60:], \n",
    "                log_pr_tr.loc[features_tr_new.dropna()[fname89].iloc[-1440*60:].index], \n",
    "                window_size=3)\n",
    "\n",
    "models['model89'] = MultiOutputRegressor(GradientBoostingRegressor(**params)).fit(features_tr_f_new.reshape(N,-1), label_tr_f_new[:,:,[8,9]].reshape(-1, 2))\n",
    "print('model 8,9 score = %.2f'%models['model89'].score(features_tr_f_new.reshape(N,-1), label_tr_f_new[:,:,[8,9]].reshape(-1, 2)))\n",
    "\n",
    "\n",
    "\n",
    "fname1 = np.union1d(selected_cor[1], selected_mi[1])\n",
    "fname1 = features_tr.columns[fname1]\n",
    "fname6 = np.union1d(selected_cor[6], selected_mi[6])\n",
    "fname6 = features_tr.columns[fname6]\n",
    "fname16 = np.union1d(fname1, fname6)\n",
    "\n",
    "features_tr_f_new, label_tr_f_new = formulize_data(\n",
    "                features_tr_new.dropna()[fname16].iloc[-1440*60:], \n",
    "                log_pr_tr.loc[features_tr_new.dropna()[fname16].iloc[-1440*60:].index], \n",
    "                window_size=3)\n",
    "\n",
    "models['model16'] = MultiOutputRegressor(GradientBoostingRegressor(**params)).fit(features_tr_f_new.reshape(N,-1), label_tr_f_new[:,:,[1,6]].reshape(-1, 2))\n",
    "print('model 1,6 score = %.2f'%models['model16'].score(features_tr_f_new.reshape(N,-1), label_tr_f_new[:,:,[1,6]].reshape(-1, 2)))\n",
    "\n",
    "\n",
    "\n",
    "fname4 = np.union1d(selected_cor[4], selected_mi[4])\n",
    "fname4 = features_tr.columns[fname4]\n",
    "fname5 = np.union1d(selected_cor[5], selected_mi[5])\n",
    "fname5 = features_tr.columns[fname5]\n",
    "fname7 = np.union1d(selected_cor[7], selected_mi[7])\n",
    "fname7 = features_tr.columns[fname7]\n",
    "fname457 = np.union1d(fname4, fname5)\n",
    "fname457 = np.union1d(fname457, fname7)\n",
    "\n",
    "features_tr_f_new, label_tr_f_new = formulize_data(\n",
    "                features_tr_new.dropna()[fname457].iloc[-1440*60:], \n",
    "                log_pr_tr.loc[features_tr_new.dropna()[fname457].iloc[-1440*60:].index], \n",
    "                window_size=3)\n",
    "\n",
    "models['model457'] = MultiOutputRegressor(GradientBoostingRegressor(**params)).fit(features_tr_f_new.reshape(N,-1), label_tr_f_new[:,:,[4,5,7]].reshape(-1, 3))\n",
    "print('model 4,5,7 score = %.2f'%models['model457'].score(features_tr_f_new.reshape(N,-1), label_tr_f_new[:,:,[4,5,7]].reshape(-1, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2412e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85baa9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./model.pkl\", 'wb') as f:\n",
    "    pickle.dump(models, f)\n",
    "\n",
    "feature_names_dict = {'fname0':fname0, 'fname2':fname2, 'fname3': fname3, 'fname16': fname16, 'fname89': fname89, 'fname457': fname457}\n",
    "with open('./fname.pkl', 'wb') as f:\n",
    "    pickle.dump(feature_names_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b125aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7805/7805 [04:49<00:00, 26.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time used: 289.405s\n",
      "Pairwise correlation:\n",
      "\tasset 0 = 0.00725\n",
      "\tasset 1 = 0.04308\n",
      "\tasset 2 = 0.02865\n",
      "\tasset 3 = 0.03738\n",
      "\tasset 4 = 0.04710\n",
      "\tasset 5 = 0.01897\n",
      "\tasset 6 = 0.04310\n",
      "\tasset 7 = 0.02381\n",
      "\tasset 8 = 0.04031\n",
      "\tasset 9 = 0.03296\n",
      "\tmean correlation = 0.03226\n",
      "Overall correlation: -0.00729\n",
      "===============================\n",
      "Fail to outperform Ziwei's method, whose pairwise average\n",
      "and overall correlations are (0.02840, 0.01536)\n",
      "===============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(289.40489983558655,\n",
       " 0    0.007253\n",
       " 1    0.043080\n",
       " 2    0.028653\n",
       " 3    0.037376\n",
       " 4    0.047102\n",
       " 5    0.018972\n",
       " 6    0.043104\n",
       " 7    0.023815\n",
       " 8    0.040313\n",
       " 9    0.032957\n",
       " dtype: float64,\n",
       " -0.007294364737689579)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_r_hat(A, B):\n",
    "    features = get_features(A, B).dropna()\n",
    "    pred = np.zeros(10)\n",
    "\n",
    "    features0 = features[feature_names_dict['fname0']].values[-3:]\n",
    "    pred[0] = models['model0'].predict(features0.reshape(1,-1))\n",
    "\n",
    "    features2 = features[feature_names_dict['fname2']].values[-3:]\n",
    "    pred[2] = models['model2'].predict(features2.reshape(1,-1))\n",
    "\n",
    "    features3 = features[feature_names_dict['fname3']].values[-3:]\n",
    "    pred[3] = models['model3'].predict(features3.reshape(1,-1))\n",
    "\n",
    "    features16 = features[feature_names_dict['fname16']].values[-3:]\n",
    "    pred[[1,6]] = models['model16'].predict(features16.reshape(1,-1))\n",
    "\n",
    "    features89 = features[feature_names_dict['fname89']].values[-3:]\n",
    "    pred[[8,9]] = models['model89'].predict(features89.reshape(1,-1))\n",
    "\n",
    "    features457 = features[feature_names_dict['fname457']].values[-3:]\n",
    "    pred[[4,5,7]] = models['model457'].predict(features457.reshape(1,-1))\n",
    "\n",
    "    return pred - A.values[-1]\n",
    "\n",
    "import critic\n",
    "cr = critic.Critic()\n",
    "cr.submit(get_r_hat, log_pr_tst, volu_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61122de5",
   "metadata": {},
   "source": [
    "Total time used: 289.405s\n",
    "Pairwise correlation:\n",
    "\tasset 0 = 0.00725\n",
    "\tasset 1 = 0.04308\n",
    "\tasset 2 = 0.02865\n",
    "\tasset 3 = 0.03738\n",
    "\tasset 4 = 0.04710\n",
    "\tasset 5 = 0.01897\n",
    "\tasset 6 = 0.04310\n",
    "\tasset 7 = 0.02381\n",
    "\tasset 8 = 0.04031\n",
    "\tasset 9 = 0.03296\n",
    "\tmean correlation = 0.03226\n",
    "Overall correlation: -0.00729\n",
    "===============================\n",
    "Fail to outperform Ziwei's method, whose pairwise average\n",
    "and overall correlations are (0.02840, 0.01536)\n",
    "===============================\n",
    "\n",
    "(289.40489983558655,\n",
    " 0    0.007253\n",
    " 1    0.043080\n",
    " 2    0.028653\n",
    " 3    0.037376\n",
    " 4    0.047102\n",
    " 5    0.018972\n",
    " 6    0.043104\n",
    " 7    0.023815\n",
    " 8    0.040313\n",
    " 9    0.032957\n",
    " dtype: float64,\n",
    " -0.007294364737689579)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b993c74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
